{
  "createArticle": "+ Create Article",
  "newsletterFormHeading": "Join Our Newsletter",
  "newsletterFormDescription": "Sign up for Metaculus news, special forecasting reports, and event invitations.",
  "predict": "Predict",
  "withdraw": "Withdraw",
  "withdrawForecast": "Withdraw Forecast",
  "saveChange": "Save Change",
  "reaffirm": "Reaffirm",
  "rescalePrediction": "Auto-sum to 100%",
  "signUpToPredict": "Sign Up to Predict",
  "logIn": "Log in",
  "tutorial": "Tutorial",
  "logOut": "Log Out",
  "tweetPrediction": "Tweet Prediction!",
  "postPredictionAsComment": "Post Prediction as Comment",
  "predictionUpcomingMessage": "This question is in the Upcoming stage, and will be open for predictions soon",
  "predictionUnapprovedMessage": "This question must be approved by moderators before you can begin predicting",
  "predictionClosedMessage": "This question is closed for predictions, and is waiting to be resolved",
  "predictionConditionalPartiallyClosedMessage": "The question \"{closedQuestion}\" has closed, so these conditionals are closed for forecasting. They will resolve when the question \"{activeQuestion}\" resolves.",
  "resolve": "Resolve",
  "unresolve": "Unresolve",
  "preview": "Preview",
  "updatePreviewButton": "Update Preview",
  "postButton": "Post",
  "loadMoreButton": "Load More",
  "addComponentButton": "Add Component",
  "addAnother": "Add another",
  "addOption": "Add Option",
  "adminEditButton": "admin edit",
  "aggregationExplorer": "Aggregation Explorer",
  "chooseAggregationsToInclude": "Choose aggregations to include (all included by default)",
  "remove": "remove",
  "removed": "removed",
  "reply": "Reply",
  "cancel": "Cancel",
  "edit": "Edit",
  "report": "Report",
  "delete": "Delete",
  "copyLink": "Copy Link",
  "copyId": "Copy ID",
  "continueButton": "Continue",
  "submit": "Submit",
  "submitTagsFeedback": "Submit Tags Feedback",
  "discardChangesButton": "Discard Changes",
  "opens": "Opens",
  "opened": "Opened",
  "closes": "Closes",
  "closed": "Closed",
  "pending": "pending",
  "resolves": "Resolves",
  "resolved": "Resolved",
  "scheduledResolution": "Scheduled resolution",
  "scheduledCloseTime": "scheduled close time",
  "forDiscussion": "for discussion",
  "Submitted": "Submitted",
  "inReview": "In Review",
  "Deferred": "Deferred",
  "deleted": "Deleted",
  "Draft": "Draft",
  "Upcoming": "Upcoming",
  "Open": "Open",
  "Closed": "Closed",
  "Resolved": "Resolved",
  "count": "Count",
  "image": "Image",
  "resolutionAmbiguous": "Ambiguous",
  "resolutionAnnulled": "Annulled",
  "resolutionPending": "Pending resolution",
  "resolution": "resolution",
  "pendingResolution": "pending resolution",
  "forecastTimelineHeading": "Forecast Timeline",
  "totalPredictionsLabel": "Total Predictions",
  "totalForecastersLabel": "Total Forecasters",
  "cpRevealed": "Revealed",
  "communityPredictionLabel": "Community Prediction",
  "metaculusPredictionLabel": "Metaculus Prediction",
  "userPredictionLabel": "User Prediction",
  "myPrediction": "My Prediction",
  "myPredictionPrevious": "My Prediction (previous)",
  "myPredictionValue": "My Prediction: <forecast>{forecastValue}</forecast>",
  "myCoverageLabel": "My Coverage",
  "predictionLabel": "Prediction",
  "pdfLabel": "Probability Density",
  "cdfLabel": "Cumulative Probability",
  "pdf": "PDF",
  "cdf": "CDF",
  "recencyWeighted": "Recency weighted",
  "unweighted": "Unweighted",
  "metaculus": "Metaculus",
  "community": "community",
  "firstQuartile": "lower 25%",
  "secondQuartile": "median",
  "thirdQuartile": "upper 75%",
  "you": "you",
  "youPrevious": "you (previous)",
  "overlayMostRecentForecast": "Overlay Most Recent Forecast",
  "overlayCurrentForecast": "Overlay Current Forecast",
  "resolutionDescriptionBinary": "Did this actually happen?",
  "resolutionDescriptionMultipleChoice": "Which of these actually happened?",
  "resolutionDescriptionContinuous": "What was the final result?",
  "resolutionRelativeLogScore": "My relative log score:",
  "resolutionAbsoluteLogScore": "My absolute log score:",
  "communityAbsoluteLogScore": "Community absolute log score:",
  "metaculusAbsoluteLogScore": "Metaculus absolute log score:",
  "metaculusRelativeLogScore": "Metaculus relative log score:",
  "myBaselineScore": "My Baseline Score",
  "myPeerScore": "My Peer Score",
  "mySpotBaselineScore": "My Spot Baseline Score",
  "mySpotPeerScore": "My Spot Score",
  "myRelativeLegacyScore": "My Relative Legacy Score",
  "myRelativeLegacyArchivedScore": "My Relative Legacy Archived Score",
  "communityBaselineScore": "Community Baseline Score",
  "communityPeerScore": "Community Peer Score",
  "communnitySpotBaselineScore": "Community Spot Baseline Score",
  "communnitySpotPeerScore": "Community Spot Score",
  "communnityRelativeLegacyScore": "Community Relative Legacy Score",
  "communnityRelativeLegacyArchivedScore": "Community Relative Legacy Archived Score",
  "myCoverage": "My coverage",
  "myWeightedCoverage": "My weighted coverage",
  "questionWillOpenAt": "This question will open {{date}}.",
  "questionNotOpenYet": "This question is not yet open for predictions.",
  "youMustLogInToComment": "You must log in to comment",
  "commentsReadGuidelines": "Read our guidelines",
  "commentsLearnMarkdown": "Learn about our Markdown",
  "commentDeleted": "Comment deleted",
  "commentsReportCommentHeading": "Report this comment",
  "commentsReportCommentDescription": "Is there something wrong with this comment? Please help us keep our community great by reporting it.",
  "commentsReportSpam": "Report as spam",
  "commentsReportGuidelinesViolation": "Report as violating community guidelines",
  "commentsMyPrediction": "My prediction",
  "commentsReportSubmittedMessage": "Report submitted, thank you!",
  "replied": "replied",
  "commented": "commented",
  "inReplyTo": "In reply to {author}",
  "commentTooLateToUnvoteError": "Cannot undo votes on comments from previous calendar years",
  "hide": "Hide",
  "show": "Show",
  "showMore": "Show More",
  "showMoreTags": "Show more tags",
  "showLess": "Show Less",
  "showMoreNews": "Show More News",
  "showMoreQuestions": "Show More Questions",
  "showReplyWithCount": "show {count} replies",
  "hideReplyWithCount": "hide {count} replies",
  "now": "now",
  "weight": "weight",
  "recent": "recent",
  "uniform": "uniform",
  "median": "median",
  "mean": "mean",
  "scoreIfNo": "Score if No",
  "scoreIfYes": "Score if Yes",
  "ifNo": "if no",
  "ifYes": "if yes",
  "ifNO": "if No",
  "ifYES": "if Yes",
  "absoluteLogScore": "Absolute Log Score",
  "brierScore": "Brier Score",
  "yes": "yes",
  "Yes": "Yes",
  "no": "no",
  "fanGraphFirstQuartileLabel": "25th percentile",
  "fanGraphSecondQuartileLabel": "Median",
  "fanGraphThirdQuartileLabel": "75th percentile",
  "questionGroupTableFirstQuartileLabel": "25th",
  "questionGroupTableSecondQuartileLabel": "median",
  "questionGroupTableThirdQuartileLabel": "75th",
  "finePrint": "Fine Print",
  "hideSpoiler": "Hide spoiler",
  "showSpoiler": "Show spoiler",
  "commentHidden": "This comment has been hidden",
  "notifyMe": "Notify Me",
  "showCommunityPredictionByDefault": "Show Community Prediction by default on open questions",
  "showCommunityPredictionIfPredicted": "Show Community Prediction if you've predicted",
  "showCommunityPrediction": "Show Community Prediction",
  "hideCommunityPrediction": "Hide Community Prediction",
  "viewMetaculusPrediction": "View Metaculus Prediction",
  "questionSearchPlaceholder": "search questions...",
  "articlesSearchPlaceholder": "search articles...",
  "tagSearchPlaceholder": "search tags...",
  "search": "Search",
  "searchFilterType": "Type",
  "searchFilterForecastStatus": "Forecast Status",
  "searchFilterParticipation": "My Participation",
  "searchFilterOrderBy": "Order By",
  "searchOptionAll": "All",
  "searchOptionForecast": "Forecast",
  "searchOptionDiscussion": "Discussion",
  "searchOptionAllAccepted": "All Accepted",
  "searchOptionPending": "Pending",
  "searchOptionDeferred": "Deferred",
  "searchOptionUnmoderated": "Unmoderated",
  "searchOptionUpcoming": "Upcoming",
  "searchOptionOpen": "Open",
  "searchOptionClosed": "Closed",
  "searchOptionResolved": "Resolved",
  "searchOptionDraft": "Draft",
  "searchOptionAny": "Any",
  "searchOptionPredicted": "Predicted",
  "searchOptionActivePrediction": "Active Prediction",
  "searchOptionWithdrawnPrediction": "Withdrawn",
  "searchOptionNotPredicted": "Not Predicted",
  "searchOptionAuthored": "Authored",
  "searchOptionUpvoted": "Upvoted",
  "searchOptionPrivate": "Private",
  "searchOptionModerating": "Moderating",
  "searchOptionActivity": "Activity",
  "searchOptionUpvotes": "Upvotes",
  "searchOptionNewest": "Newest",
  "searchOptionRecentlyOpened": "Recently Opened",
  "searchOptionSoonestOpening": "Soonest Opening",
  "searchOptionSoonestClosing": "Soonest Closing",
  "searchOptionRecentlyClosed": "Recently Closed",
  "searchOptionSoonestResolving": "Soonest Resolving",
  "searchOptionRecentlyResolved": "Recently Resolved",
  "searchOptionOldestPrediction": "Oldest Prediction",
  "searchOptionRelevance": "Relevance",
  "searchOptionFewestComments": "Fewest Comments",
  "searchOptionDateSubmitted": "Date Submitted",
  "createdBy": "Created by {{author}}",
  "onDate": "on {date}",
  "published": "Published",
  "byAuthor": "Authored by {{author}}",
  "estimatedReadingTime": "{minutes} min read",
  "predictions": "Predictions",
  "predictors": "Predictors",
  "relativeLog": "Relative Log",
  "unreadAll": "all unread",
  "questionsLeft": "{count, plural, =1 {1 question left} other {{count} questions left} }",
  "crowdMedian": "Crowd Median",
  "respondCrowdMedian": "Please respond with your answer, then what you think the crowd median in this room will be.",
  "commentsWithCount": "{count, plural, =0 {no comments} =1 {comment} other {comments} }",
  "unreadWithCount": "{count, plural, =0 {no unread} other {unread} }",
  "forecastersWithCount": "{count, plural, =1 {forecaster} other {forecasters} }",
  "or": "or",
  "registrationHeadingPrediction": "Make your prediction count",
  "registrationHeadingSite": "Create a Metaculus Account",
  "registrationInfoParagraph1": "Register as a Metaculus user and you'll be able to:",
  "registrationInfoParagraph2": "You can easily register by email or social media.",
  "registrationInfoAbility1": "Keep track of your predictions and build a track record",
  "registrationInfoAbility2": "Get notified when predictions that you care about change",
  "registrationInfoAbility3": "Join the discussion with other forecasters",
  "registrationInfoAbility4": "Create private questions to track personal predictions, and optionally share them with your friends",
  "registrationInfoAbility5": "Write new public questions that you want to see answered",
  "registrationFacebook": "Continue with Facebook",
  "registrationGoogle": "Continue with Google",
  "registrationByEmail": "Sign Up",
  "emptyOptionError": "option can't be empty",
  "errorRequired": "required field",
  "errorMinLength": "{field} must be at least {minLength} characters",
  "errorMaxLength": "{field} must be at most {maxLength} characters",
  "errorPasswordMatch": "passwords must match",
  "errorEmailInvalid": "not a valid email",
  "errorGeneric": "Error, something went wrong. Please check your connection and try again or contact us at support@metaculus.com",
  "registrationUsernamePlaceholder": "choose a username",
  "passwordPlaceholder": "password",
  "registrationVerifyPasswordPlaceholder": "verify password",
  "registrationEmailPlaceholder": "email",
  "registrationSignInHeading": "Already have an account?",
  "loginSignUpHeading": "Don't have an account yet?",
  "createAnAccount": "Sign Up",
  "registrationSuccessHeading": "Activation Email Sent",
  "registrationSuccess1": "Thanks for creating a Metaculus account <username></username>!",
  "registrationSuccess2": "An activation email has been sent to <email></email>. Please follow the link inside to finish creating your account.",
  "registrationTerms": "By registering, you acknowledge and agree to Metaculus's <terms>Terms of Use</terms> and <privacy>Privacy Policy</privacy>.",
  "registrationTermsAgreement": "I agree — continue registration",
  "loginFacebook": "Log in with Facebook",
  "loginGoogle": "Log in with Google",
  "loginUsernamePlaceholder": "username or email",
  "forgotPasswordLink": "Forgot Password?",
  "passwordResetHeading": "Password Reset",
  "passwordResetDescription": "Enter your username and we'll send you an email to reset your password.",
  "resetPasswordButton": "Reset Password",
  "resetPasswordEmailSentHeading": "Password Reset Email Sent",
  "resetPasswordEmailSent1": "Your password reset request has been successfully processed.",
  "resetPasswordEmailSent2": "An email has been sent to the address on record for that account. Please follow the link inside to reset your password.",
  "newPasswordLabel": "new password",
  "verifyPasswordLabel": "verify password",
  "metaculusSupporterHeading": "Metaculus Supporter",
  "supporterSinceHeading": "Supporter Since",
  "changeUsernameHeading": "Change your username",
  "changeUsernameDescription": "You can change your username once every six months. Your old username will be listed as an alias in your profile for 30 days.",
  "newUsernamePlaceholder": "new username",
  "confirm": "Confirm",
  "confirmUsernamePlaceholder": "confirm username",
  "confirmUnresolveQuestion": "Are you sure you want to unresolve this question?",
  "usernamesNotMatching": "usernames don't match",
  "projectContents": "Project Contents",
  "question": "Question",
  "created": "Created",
  "currentPasswordPlaceholder": "Enter current password...",
  "newPasswordPlaceholder": "Enter new password...",
  "verifyPasswordPlaceholder": "Verify new password...",
  "updatePasswordButton": "Update Password",
  "passwordChangeSuccess": "password successfully changed",
  "setReminderHeading": "Schedule a notification",
  "remindWhen": "Trigger",
  "repeatingColumnTitle": "Repeating",
  "repeatsNo": "No",
  "repeatsYes": "Yes",
  "remindOnDate": "On {{date}}",
  "remindWhenOpens": "Question opens",
  "remindWhenCPChanges": "Community Prediction changes 10%",
  "remindWhenNeedsResolution": "Question needs resolution",
  "remindWhenPrcLifetime": "{{percent}}% of question lifetime elapsed",
  "remindWhenTimeElapsed": "On {{date}}",
  "noReminder": "(no scheduled notifications)",
  "setReminderDescription": "Get email notifications when important updates happen.",
  "selectTriggerLabel": "Select trigger",
  "reminderOptionDate": "On date",
  "reminderOptionTimeElapsed": "Time elapsed",
  "reminderOptionNewComments": "New comments",
  "reminderOptionNeedsResolution": "Question needs resolution",
  "reminderOptionCPChanges": "Community prediction changes",
  "reminderOptionQuestionOpens": "Question opens",
  "reminderOptionQuestionLifetime": "Question lifetime percentage",
  "loadingRemindersError": "Could not load the notification",
  "removingReminderError": "Could not remove the notification",
  "removeReminderConfirmation": "Do you really want to remove this notification?",
  "newReminderSuccess": "Notification has been scheduled",
  "newReminderError": "Error, notification has not been scheduled",
  "reminderErrorTypeRequired": "Please select a trigger",
  "reminderErrorDateRequired": "Please select a date",
  "reminderErrorNotifyInThePast": "We cannot notify you in the past (yet)",
  "reminderErrorNumberOfCommentsRequired": "To reduce spam, we cannot notify you for less than 1 comment",
  "reminderErrorNumberOfCommentsTooBig": "The number of comments should be less than 100",
  "reminderErrorPercentLifetimeRequired": "Please set a percent of lifetime",
  "reminderErrorPercentLifetimeCannotHappen": "This would result in no notification, please select a lower value",
  "reminderErrorPercentLifetimeWouldNotRepeat": "This would result in no repetition. Consider switching to \"Doesn't Repeat\"",
  "reminderNotePlaceholder": "Write yourself a note...",
  "reminderCPChangesDescription1": "Notify me when the forecast changes by more than 10%",
  "reminderCPChangesDescription2": "For continuous questions, this is measured by a <a href=\"https://en.wikipedia.org/wiki/Kolmogorov–Smirnov_test\">Kolmogorov–Smirnov test</a>.",
  "reminderDateDescription": "Notify me on",
  "reminderNewCommentsDescription1": "Notify me after ",
  "reminderNewCommentsDescription2_one": "new comment",
  "reminderNewCommentsDescription2_few": "new comment",
  "reminderNewCommentsDescription2_other": "new comments",
  "reminderNewCommentsDescription2": "new comments",
  "reminderQResolutionDescription": "Notify me when this question needs resolution",
  "reminderPercentLifeDescription1": "Notify me when",
  "reminderPercentLifeDescription2": "% of the question lifetime has elapsed",
  "reminderPercentLifeCurrentValue": "This question is currently at {{percent}} % of its lifetime",
  "reminderTimeElapsedDescription": "Notify me in",
  "reminderTimeElapsedADay": "a day",
  "reminderTimeElapsedAWeek": "a week",
  "reminderTimeElapsedAMonth": "a month",
  "reminderTimeElapsedAYear": "a year",
  "reminderPercentLifetimeDescription": "Notify after x percent of question lifetime",
  "submitReminderButton": "Schedule",
  "reminderLabelNoRepeat": "Doesn't repeat",
  "reminderLabelRepeat": "Repeats",
  "reminderLabelDaily": "Daily",
  "reminderLabelWeekly": "Weekly",
  "reminderLabelMonthly": "Monthly",
  "reminderLabelAnnually": "Annually",
  "yourScheduledNotifications": "Your scheduled notifications for this question",
  "trackRecordNoUserPredictions": "None of your predictions have resolved yet! Make more predictions to build up your track record.",
  "mpExplanation": "You have special access to see the Metaculus Prediction early.",
  "cpVisibilitySettingOn": "You've hidden the Community Prediction in your <2>settings</2>",
  "cpInHiddenPeriod": "The Community Prediction will be <2>revealed</2> ",
  "duration": "duration",
  "ending": "ending",
  "forecasts": "Forecasts",
  "user": "User",
  "viewProfile": "View Profile",
  "total": "Total",
  "average": "Average",
  "score": "Score",
  "coverage": "Coverage",
  "totalCoverage": "Total Coverage",
  "totalScore": "Total Score",
  "totalTake": "Total Take",
  "weightedAverageScore": "Weighted Average Score",
  "hIndex": "H-Index",
  "upvotes": "Upvotes",
  "forecasters": "Forecasters",
  "forecaster": "Forecaster",
  "comment": "Comment",
  "questionType": "Question Type",
  "unsupportedQuestionType": "Unsupported question type",
  "questionStatus": "Question Status",
  "similarQuestions": "Similar Questions",
  "category": "Category",
  "tags": "Tags",
  "tagFilter": "Tag: {tag}",
  "questionAuthor": "Question Author",
  "questionAuthorFilter": "Author: {author}",
  "myParticipation": "My Participation",
  "year": "year",
  "years": "years",
  "error": "Error",
  "loading": "Loading",
  "leaderboards": "Leaderboards",
  "binary": "Binary",
  "conditional": "conditional",
  "group": "Group",
  "multipleChoice": "Multiple Choice",
  "continuous": "Continuous",
  "baselineScore": "Baseline Score",
  "baselineAccuracy": "Baseline Accuracy",
  "baselineAccuracyShort": "Baseline Accuracy",
  "peerScore": "Peer Score",
  "peerAccuracy": "Peer Accuracy",
  "peerAccuracyShort": "Peer Accuracy",
  "insight": "Insight",
  "questionWriting": "Question Writing",
  "questionWritingShort": "Question Writing",
  "relativeAccuracy": "Relative Accuracy",
  "absoluteAccuracy": "Absolute Accuracy",
  "rank": "Rank",
  "outOfRank": "out of {total}",
  "comments": "Comments",
  "questions": "Questions",
  "viewMore": "View more...",
  "randomQuestion": "Random Question",
  "notebooks": "Notebooks",
  "otherWithCount": "{count, plural, =1 {# other} other {# others} }",
  "feedHome": "Feed Home",
  "topics": "Topics",
  "categories": "categories",
  "Categories": "Categories",
  "seeAllCategories": "See all categories",
  "toggleAllTopics": "Toggle all topics",
  "Filter": "Filter",
  "done": "done",
  "clear": "clear",
  "predicted": "Predicted",
  "notPredicted": "Not Predicted",
  "author": "Author",
  "authorWithCount": "{count, plural, =1 {Author} other {Authors} }",
  "actions": "actions",
  "questionWeight": "Question Weight",
  "questionWeightTooltip": "This question has a weight of <bold>{weight}%</bold>, providing {count, plural, =1 {<bold>{weightDiff}% fewer points</bold>} other {a <bold>{weightDiff}% point boost</bold>}} compared to regular Metaculus questions.",
  "includeBots": "Include Bots",
  "includeBotsTooltip": "For this question, bot forecasts contribute to the Community Prediction. <link>Learn more about AI Benchmarking</link>",
  "authored": "Authored",
  "upvoted": "Upvoted",
  "moderating": "Moderating",
  "moderatedBy": "Moderated by",
  "visibility": "Visibility",
  "public": "Public",
  "unlisted": "Unlisted",
  "draft": "Draft",
  "private": "Private",
  "hot": "Hot",
  "movers": "Movers",
  "new": "New",
  "oldest": "Oldest",
  "newest": "Newest",
  "stale": "Stale",
  "newComments": "New Comments",
  "divergence": "Divergence",
  "myPredictions": "My Predictions",
  "special": "Special",
  "personal": "Personal",
  "myQuestionsAndPosts": "My Questions & Posts",
  "mostUpvotes": "Most Upvotes",
  "mostComments": "Most Comments",
  "totalComments": "Total Comments",
  "mostPredictions": "Most Predictions",
  "closingSoon": "Closing Soon",
  "closingTime": "Closing Time",
  "bestScores": "Best Scores",
  "worstScores": "Worst Scores",
  "resolvingSoon": "Resolving Soon",
  "unreadComments": "Unread Comments",
  "oldestPredictions": "My Oldest Predictions",
  "newestPredictions": "My Newest Predictions",
  "recentPredictions": "Recent predictions",
  "myDivergence": "My Divergence",
  "profile": "Profile",
  "changeUsernameButton": "change",
  "markUserAsSpamButton": "Mark User as Spam",
  "markUserAsSpam?": "Are you sure you want to mark this user as spam?",
  "memberSince": "Member Since",
  "bio": "Bio",
  "website": "Website",
  "profileBioPlaceholder": "Tell us a little about yourself.",
  "username": "username",
  "introducingCommunities": "<bold>Introducing Communities</bold> – a new feature where members create, discuss, and resolve their own questions on various topics. Follow your favorite communities to stay informed, and contribute to forecasts in areas that matter to you.",
  "introducingCommunitiesContact": "Want to create your own Community? <contact>Reach out</contact> to get started.",
  "noOpenCommunityQuestions": "Your community currently does not have any questions open for forecasting",
  "noUpcomingCommunityQuestions": "Your community currently does not have any upcoming questions",
  "noClosedCommunityQuestions": "No closed questions yet",
  "noResolvedCommunityQuestions": "No questions resolved yet",
  "noReviewCommunityQuestions": "No questions in review yet",
  "noResults": "No results found",
  "noTags": "No tags found",
  "homeTitle": "Forecasting for a <highlight>complex world</highlight>",
  "homeDescription": "Metaculus offers trustworthy forecasting and modeling infrastructure for forecasters, decision makers, and the public.",
  "forecastsSearchPlaceholder": "Search Metaculus",
  "focusAreasTitle": "Focus <highlight>Areas</highlight>",
  "focusAreasDescription": "We create forecasts and advance research, decision making, and policy in four main topic areas.",
  "seeForecasts": "See Forecasts",
  "tournaments": "Tournaments",
  "joinTournaments": "Join our tournaments and predict real world events, timelines, and impacts and win prizes for accuracy.",
  "tournamentsHero1": "Help the global community model, understand, and navigate the world’s most important and complex challenges.",
  "tournamentsHero2": "Prove your forecasting abilities, support effective policy and decision-making, and win cash prizes.",
  "tournamentsHero3": "<scores>Learn</scores> how tournaments are scored.",
  "tournamentsHero4": "<email>Reach out</email> to discuss launching a tournament on what’s important to you.",
  "ActiveTournaments": "Active Tournaments",
  "Tournament": "Tournament",
  "QuestionSeries": "Question Series",
  "Archive": "Archive",
  "prizePool": "prize pool",
  "prize": "Prize",
  "percentPrize": "% Prize",
  "questionCount": "{count, plural, =1 {# Question} other {# Questions} }",
  "closesOn": "Closes <strong></strong>",
  "closedOn": "Closed <strong></strong>",
  "highestPrizePool": "Highest Prize Pool",
  "endingSoon": "Ending Soon",
  "changePasswordButton": "Change Password",
  "formerlyKnownAs": "Formerly Known As",
  "StartDate": "Start Date",
  "EndDate": "End Date",
  "SeriesContents": "Series Contents",
  "makePrediction": "Make a Prediction",
  "Title": "Title",
  "Access": "Access",
  "save": "Save",
  "discard": "Discard",
  "saveChanges": "Save Changes",
  "Candidates": "Candidates",
  "followingCommunities": "Communities You’re Following",
  "allCommunities": "All communities",
  "otherCommunities": "Other communities",
  "browserAllCommunities": "Browse all communities",
  "communities": "Communities",
  "backTo": "Back to",
  "sendBackToDrafts": "Send back to Drafts",
  "approve": "Approve",
  "apply": "Apply",
  "postQuestionApproval": "Question Approval",
  "postNotebookApproval": "Notebook Approval",
  "postQuestionApprovalSubtitle": "Make any necessary adjustments to the general settings for each question in this section.",
  "postGroupOfQuestionsApprovalSubtitle": "Set the Open Time and CP Reveal Time for all subquestions. You can adjust these settings for individual questions later during editing.",
  "postNotebookApprovalSubtitle": "Are you sure you want to approve this notebook?",
  "questionApprovalSubtitle": "Make any necessary adjustments to the general settings for each question in this section.",
  "submitForReview": "Submit for review",
  "seeAllTournaments": "See All Tournaments",
  "seeAllForecasts": "See All Forecasts",
  "seeMorePosts": "See More Posts",
  "documentation": "documentation",
  "apiAcessText": "You may use an API token to access the Metaculus API, per our",
  "apiAccess": "API Access",
  "yourAPITokenIs": "Your API token is:",
  "about": "About",
  "api": "API",
  "otherInitiatives": "Other Initiatives",
  "forecasting": "Forecasting",
  "forecastingResources": "forecasting resources",
  "forjournalists": "Metaculus for Journalists",
  "careers": "Careers",
  "engageWith": "Engage with",
  "guidelines": "Guidelines",
  "privacyPolicy": "Privacy Policy",
  "termsOfUse": "Terms of Use",
  "faq": "FAQ",
  "contact": "Contact",
  "contactUs": "Contact Us",
  "thankYouForGettingInTouch": "Thank you for getting in touch. We’ll get back to you soon!",
  "yourEmail": "Your Email",
  "yourMessage": "Your message...",
  "news": "News",
  "newsMatch": "News Match",
  "create": "create",
  "login": "Login",
  "logout": "Logout",
  "admin": "Admin",
  "aboutMetaculus": "About Metaculus",
  "aboutMetaculusTitle": "<blue>About</blue> Metaculus",
  "aboutMetaculusDescription": "Metaculus is an online forecasting platform and aggregation engine working to improve human reasoning and coordination on topics of global importance.",
  "more": "More",
  "moreLikely": "percentage points higher",
  "afterElection": "under {winner}",
  "ifCandidateElected": "if {candidate} elected",
  "electoralConsequences": "Electoral Consequences",
  "forJournalists": "For Journalists",
  "trackRecord": "Track Record",
  "medals": "Medals",
  "metaculusTrackRecord": "Metaculus Track Record",
  "trackRecordOutdatedMessage": "This track record does not yet use the new Baseline and Peer scores <link>introduced in November 2023</link>. Updating this page is one of our next priorities.",
  "trackRecordShowStatistics": "A Track Record shows some statistics about the predictor's performance.",
  "theJournal": "The Journal",
  "account": "Account",
  "settings": "Settings",
  "shortTitle": "Short Title",
  "boost": "Boost",
  "bury": "Bury",
  "duplicate": "Duplicate",
  "downloadQuestionData": "Download Question data",
  "partnersUseForecasts": "Our partners use Metaculus forecasts to gain insight into complex challenges and decisions.",
  "learnHowYouCanPartner": "Learn how you can partner with Metaculus to host a forecasting tournament, collaborate on forecasting research, hire our <link>Pro Forecasters</link> for custom projects, or set up a private forecasting space for your organization.",
  "workingWithNonProfits": "We love working with non-profits in our core focus areas. If you’re a non-profit seeking forecasting and modeling capacity or are interested in collaborating on a grant, please reach out!",
  "reachOutToLearnMore": "Please reach out to learn more about working with Metaculus or with any questions, comments or concerns. We’ll get back to you soon.",
  "feelFreeToJustSayHello": "Feel free to just say hello, too - we love hearing from the forecasting community!",
  "contentBoosted": "Content boosted! value {score} activity. Total boost score for the week: {score_total}",
  "contentBuried": "Content buried! value {score} activity. Total boost score for the week: {score_total}",
  "downloadQuestionDataError": "Error downloading Question data",
  "cpRevealTime": "CP Reveal Time",
  "openTime": "Open Time",
  "Question": "Question",
  "editOpenAndCpRevealTimes": "Edit Open & CP Reveal Times",
  "noQuestionsResolved": "No questions have resolved yet in this period.",
  "duration:": "Duration:",
  "timePeriod": "Time Period:",
  "inReviewBox1": "You are welcome to participate in our review process by voting and commenting on questions during their development.",
  "inReviewBox2": "Questions that achieve a positive vote balance will be prioritized for Admin review and approval.",
  "inReviewBox3": "Questions that receive a significant number of negative votes will be rejected by default.",
  "inReviewBox4": "Please refer to our <link>Question writing guidelines</link> to learn what makes a good question.",
  "learnMoreAboutNewsMatch": "<link>Learn more</link> about Metaculus NewsMatch",
  "learnMoreAboutQuestionWriting": "Learn more about question writing",
  "inReviewBoxTitle": "Participate in the review process",
  "userProfile": "{username}'s profile",
  "unknownUserProfile": "User's profile",
  "userMedals": "{username}'s medals",
  "unknownUserMedals": "User's medals",
  "noMedals": "No medals yet",
  "leaderboard": "Leaderboard",
  "openLeaderboard": "Open Leaderboard",
  "live": "Live",
  "liveLeaderboardDisclaimer": "This is a live leaderboard; rankings may change on a daily basis.",
  "legacyPeerDisclaimer": "The Peer Accuracy leaderboards used slightly different math before 2024. See details <link>here</link>.",
  "namesPrediction": "<name>{username}'s</name> Prediction",
  "includeMyForecast": "include your current forecast in this comment",
  "privateComment": "private comment",
  "groupVariable": "Group Variable",
  "showFullForecast": "...show full forecast",
  "closeFullForecast": "close full forecast",
  "noComments": "no comments",
  "overview": "Overview",
  "links": "links",
  "location": "location",
  "occupation": "occupation",
  "take": "Take",
  "confirmPageLeaveMessage": "Are you sure you want to leave this page?",
  "deletedAuthor": "deleted author",
  "myScore": "My Score",
  "peerScoreInfo": "Your <link>Peer Score</link> on that question. If you see no score, then either the question has yet to resolve or you haven’t predicted it (or both!).",
  "spotPeerScoreInfo": "Your <link>Spot Peer Score</link> on that question. If you see no score, then either the question has yet to resolve or you haven’t predicted it (or both!).",
  "scoringTerminology": "Scoring Terminology",
  "totalPeerScoreInfo": "The question-weighted sum of your <link>Peer Scores</link> on all questions in the tournament (but only those that close before the end of the tournament).",
  "totalSpotPeerScoreInfo": "The question-weighted sum of your <link>Spot Peer Scores</link> on all questions in the tournament (but only those that close before the end of the tournament).",
  "manualScoreInfo": "This is a manually updated Leaderbaord. Check details for how performance is measured.",
  "peerTakeInfo": "Your Take is the square of the question-weighted sum of your peer scores or 0 if your total score is negative.",
  "spotPeerTakeInfo": "Your Take is the square of the question-weighted sum of your spot peer scores or 0 if your total score is negative.",
  "relativeScoreInfo": "Your <link>Relative Score</link> on that question. If you see no score, then either the question has yet to resolve or you haven’t predicted it (or both!).",
  "totalRelativeScoreInfo": "The question-weighted sum of your <link>Relative Scores</link> on all questions in the tournament (but only those that close before the end of the tournament).",
  "relativeCoverageInfo": "The amount of the question or tournament that you covered.",
  "questionWeightInfo": "The weight of the question in the tournament. The score you earn from this question is multiplied by this weight.",
  "relativeTakeInfo": "Your Take is your coverage times e to the power of your total score. (c*e^s)",
  "backgroundInfo": "Background Info",
  "parentBackgroundInfo": "Parent Question Background Info",
  "childBackgroundInfo": "Child Question Background Info",
  "histogram": "Histogram",
  "histogramAfterPrediction": "Histograms will be revealed once you submit your prediction.",
  "frequency": "Frequency",
  "brierScoreForPlayer": "Brier scores for player predictions",
  "scoreHistogram": "Score Histogram",
  "scoreScatterPlot": "Score Scatter Plot",
  "expand": "Expand",
  "collapse": "Collapse",
  "details": "Details",
  "resolutionCriteria": "Resolution Criteria",
  "parentResolutionCriteria": "Parent Resolution Criteria",
  "childResolutionCriteria": "Child Resolution Criteria",
  "loadMoreComments": "Load more comments",
  "followers": "Followers",
  "followed": "Followed",
  "followButton": "Follow",
  "followingButton": "Following",
  "unfollowButton": "Unfollow",
  "customiseButton": "Customise",
  "manageCommunityButton": "Manage Community",
  "communityManagement": "Community Management",
  "followModalCustomiseButton": "Customise",
  "followModalUnfollowButton": "Unfollow",
  "followModalYouAreFollowingThisQuestion": "You're now following this question!",
  "followModalYouAreFollowingThisGroup": "You're now following this group!",
  "followModalYouAreFollowingThisNotebook": "You're now following this notebook!",
  "followModalSuccessMessageDefaultNotificationQuestion": "You'll be notified of new comments, changes to the Community Prediction, every time 20% of the question lifetime has passed, and when the question opens, closes, or resolves.",
  "followModalSuccessMessageDefaultNotificationGroup": "You'll be notified of new comments, changes to the Community Prediction, every time 20% of the group lifetime has passed, and when the group opens, closes, or resolves.",
  "followModalSuccessMessageDefaultNotificationNotebook": "You'll receive notifications about new comments on this Notebook.",
  "followModalNotifyMe": "Notify me",
  "followModalEveryComment": "every comment",
  "followModalEveryNComments": "every {n} comments",
  "followModalSmallChanges": "small changes",
  "followModalMediumChanges": "medium changes",
  "followModalLargeChanges": "large changes",
  "followModalNotifyMeFor": "Notify me for",
  "followModalNotifyMeEvery": "Notify me every",
  "followModalMilestoneLifetimeParagraphQuestion": "of the question lifetime, and when it opens, closes, or resolves.",
  "followModalMilestoneLifetimeParagraphGroup": "of the group lifetime, and when it opens, closes, or resolves.",
  "followModalReminderThisQuestionOpened": "Reminder: this question opened on {published_at} and closed on {scheduled_close_time}",
  "followModalCustomiseNotifications": "Customise notifications",
  "followModalCustomiseNotificationsParagraph": "Configure email notifications when important updates happen.",
  "followModalCommunityPredictionChanges": "Community Prediction changes",
  "followModalMilestones": "Milestones",
  "followModalMilestonesNotifyEvery": "{pct} of the question lifetime",
  "followModalSpecificTime": "Specific time",
  "followModalStatusChanges": "Status changes",
  "unfollowModalTitle": "Are you sure?",
  "unfollowModalDescription": "You won't be notified about this question anymore",
  "best": "best",
  "cmmButton": "Changed my mind",
  "cmmUpdateButton": "Update",
  "cmmUpdatePredictionLink": "Update your prediction?",
  "updateYourPrediction": "Update your prediction",
  "democrat": "democrat",
  "republican": "republican",
  "numVotes": "{num} votes",
  "stateByStateForecasts": "State-by-State Forecasts",
  "republicanElectoralVote": "Republican Electoral Vote",
  "democratElectoralVote": "Democrat Electoral Vote",
  "electionHubDisclaimer": "<bold>Currently, our map focuses only on the battleground states outlined</bold> <link>in this question</link>.  We marked some states “Safe Democrat” or “Safe Republican” based on historical election data. If you believe we should include forecasts for more states, let us know!",
  "shareOnFacebook": "Share on Facebook",
  "shareOnTwitter": "Share on Twitter",
  "embed": "embed",
  "share": "share",
  "270VotesToWin": "270 votes to win",
  "expectedElectoralVotes": "Expected Electoral Votes",
  "electoralVotes": "Electoral Votes",
  "0Votes": "0 Votes",
  "538Votes": "538 Votes",
  "electionHubDescription": "Explore Metaculus forecasts for the 2024 US presidential election",
  "2024USElectionHub": "2024 US <blue>Election Hub</blue>",
  "safeParty": "Safe {party}",
  "partyWinProbability": "{party} Win Probability ↗",
  "settingsPreferences": "Preferences",
  "settingsSubscriptions": "Subscriptions",
  "nTriggers": "{n} triggers",
  "settingsQuestionNotifications": "Question Notifications",
  "settingsEmailNotifications": "Email Notifications",
  "settingsUserEmail": "User Email",
  "settingsNewEmail": "New Email",
  "settingsNewQuestionsInTournament": "New questions in tournaments I follow",
  "settingsEmailChangeDescription": "Enter a new email address and a password, and we'll send you a link to change your email.",
  "settingsChangeEmailAddress": "Change email address",
  "settingsChangeEmailAddressSuccess": "A confirmation email has been sent to your new address. Please follow the link inside to activate it.",
  "settingsMentionsInComments": "Mentions in comments",
  "settingsQuestionResolution": "Resolved Questions",
  "settingsSignificantMovementOnPredictedQuestions": "Significant movement on predicted questions",
  "settingsSignificantMovementOnPredictedQuestionsTooltip": "You will receive a notification email when the Community Prediction changes by more than 10% on any question you've predicted. Emails are batched every hour.",
  "newsLetter": "Newsletter",
  "research": "Research",
  "updates": "Updates",
  "posts": "posts",
  "notebook": "notebook",
  "existingQuestion": "Existing Question",
  "questionId": "Question ID",
  "addQuestion": "Add Question",
  "addExistingQuestion": "Add Existing Question",
  "existingQuestionExample": "Add an existing Metaculus question to your Community",
  "notebookExample": "text-based content that is not a question",
  "conditionalPair": "conditional pair",
  "conditionalPairExample": "If it rains today, will it rain tomorrow?",
  "dateGroup": "date group",
  "dateGroupExample": "When will it rain for the following cities?",
  "numericGroup": "numeric group",
  "numericGroupExample": "How much will it rain for these cities?",
  "binaryGroup": "binary group",
  "binaryGroupExample": "Will it rain in these cities today?",
  "binaryQuestion": "binary question",
  "binaryQuestionExample": "Will it rain today?",
  "numericQuestion": "numeric question",
  "numericRange": "numeric range",
  "numericRangeExample": "How much rain next month?",
  "dateRange": "date range",
  "dateRangeExample": "When will it rain?",
  "multipleChoiceExample": "Which city will get the most rain?",
  "questionGroup": "question group",
  "singleQuestion": "single question",
  "createNewContent": "create new content",
  "onlyRegisterUserCanCreate": "Only registered users can create new questions. <link1>Sign in</link1> or <link2>register</link2> a new account now!",
  "createQuestionDescription1": "Check out our <link1>question writing guide</link1> for tips. Good questions are approved faster, and get more predictions.  If you just have an idea, or don't want to fill out the whole submission form, post your idea in our <link2>suggestion thread</link2>.",
  "createQuestionDescription2": "We have high standards for question quality. We also favor questions on our core topic areas or that we otherwise judge valuable. We may not publish questions that are not a good fit.",
  "binaryQuestionDescription": "Binary questions are generally of the form \"Will X happen?\" They either resolve as Yes or No.",
  "multipleChoiceDescription": "Multiple choice questions are generally of the form \"Which of the following will happen?\".  These questions list many options, only one of which can resolve as true.",
  "dateRangeDescription": "A date question asks when something will happen, given a specific time window.",
  "numericRangeDescription": "A numeric question asks about a quantity or number which will be known at a future date.",
  "viewInDjangoAdmin": "View in django admin",
  "finePrintDescription": "Optional: Use the fine print for any sort of lawyerly details which don't need to be prominently displayed.",
  "createQuestion": "create question",
  "editQuestion": "Save Edits",
  "longTitle": "long title",
  "longTitleExplanation": "A single sentence that encapsulates what you're trying to predict.  Ends in a question mark.",
  "shortTitleExplanation": "This should be a shorter version of the Long Title, used where there is less space to display a title. It should end with a question mark. Examples: \"NASA 2022 spacesuit contract winner?\" or \"EU GDP from 2025 to 2035?\"",
  "backgroundInformation": "Background Information",
  "backgroundInfoExplanation": "Provide background information for your question in a factual and unbiased tone. Links should be added to relevant and helpful resources using <link>markdown syntax</link>: <markdown>[Link title](https://link-url.com)</markdown>.",
  "resolutionCriteriaExplanation": "A good question will almost always resolve unambiguously. If you have a data source by which the question will resolve, link to it here. If there is some simple math that will need to be done to resolve this question, define the equation in markdown: <markdown>\\[ y = ax^2+b \\]</markdown>.",
  "closingDate": "closing date",
  "resolvingDate": "resolving date",
  "choices": "Choices",
  "choicesSeparatedBy": "Choices (separated by ,)",
  "projects": "projects",
  "FABPrizePool": "PRIZE POOL",
  "FABPrizeValue": "$30,000",
  "FABStartDate": "START DATE",
  "FABEndDate": "ENDED ON",
  "FABStartDateJuly8": "July 8",
  "FABStartDateOct8": "October 8",
  "FABStartDateJan20": "January 20",
  "FABEndDateSep30": "September 30",
  "FABEndDateDec31": "December 31",
  "FABdesc1": "Compete with API credits provided courtesy of OpenAI and Anthropic.",
  "FABdesc2": "Features more question types, including <bold>Multiple Choice</bold>, <bold>Numerical</bold>, and <bold>Date</bold> questions.",
  "FABdesc3": "New questions open at random times for only 1-2 hours windows, to build confidence that there is no human in the loop.",
  "FABRulesTitle": "Basic rules",
  "FABRulesRule1": "No human in the loop",
  "FABRulesRule2": "Bots must post a comment explaining reasoning alongside each forecast",
  "FABRulesRule3": "One bot per user",
  "FABRulesRule4": "Bot makers must be willing to share code or a description of how their bot works, and allow a member of Metaculus to inspect their code.",
  "FABdesc4": "Bot performance benchmarked against the Metaculus community and Metaculus Pro Forecasters.",
  "FABHeroTitle": "AI Forecasting",
  "FABHeroSubtitle": "Benchmark Series",
  "FABHeroDesc": "Benchmarking the state of the art in AI forecasting against the best humans on real-world questions.",
  "FABGettingStarted": "Getting Started",
  "FABRegisterBot": "Register your bot for the tournament",
  "FABCreateBot": "Create a Bot Account",
  "FABSeparateBotAccount": "Your bot needs a separate Metaculus account. Make sure to log out of your main account and come back to this page.",
  "FABBotRegistered": "Your bot is successfully registered for the tournament.",
  "FABShowToken": "Show My Token",
  "FABTournamentPage": "The Questions",
  "FABHowItWorks": "Getting Started",
  "FABCreateBotAccount": "Create a Bot Account",
  "FABBotAlreadyCreated": "Create a bot account to participate. If you already have one, close this window, click the 'Log in' button in the upper-right corner of this page, and log in with your bot account.",
  "FABTokenInfo": "Your token is needed for your bot to interact with the Metaculus API.",
  "FABQ3Conclude": "Q3 is concluded, but Q4 is underway.",
  "FABQ4Conclude": "2025's Q1 tournament is underway!",
  "FABQ3ConcludeButton": "Click to learn more and get started",
  "FABQ4Start": "Q4 has started, but the Q3 tournament has concluded! Click for the final rankings.",
  "FABQ1Start": "Check out the final rankings for last year's Q4 tournament!",
  "inReviewStatusBox1": "This question now needs to be approved by a Metaculus Moderator. Questions that conform to our <link1>Question Writing Guidelines</link1>, and concern one of our <link2>core topics</link2>, have better chances of being approved, and faster.",
  "inReviewStatusBox2": "All Metaculus users are encouraged to give feedback on questions during Review, and upvote the questions they find interesting and/or well written. Questions with a positive vote balance will receive priority attention from Administrators. Questions with a significantly negative vote balance will be rejected by default.",
  "inReviewStatusBox4": "Note that this question cannot be approved until both its parent and child have been approved.",
  "inCommunityReviewStatus1": "This question needs approval from a Community curator like you. You can also make edits, tag the author in a comment below, send the question back to Drafts for revision, or reject the question.Some suggestions for high-quality questions:",
  "inCommunityReviewStatus2": "<li>Define terms and be concrete. Links are helpful.</li> <li>Questions should be simple to resolve and not require a great deal of admin effort.</li> <li>The title and wording of the question should align with the resolution criteria.</li> <li>Try this test: People in the future agree on how this question resolved. If they can’t, it may need clarification</li>",
  "draftStatusBox1": "Your submission is now a Draft.",
  "draftStatusBox2": "Once it's ready, please submit your draft for review by our team of Community Moderators. Thank you!",
  "newSubquestion": "new subquestion",
  "bulkEdit": "Bulk Edit",
  "bulkEditDescription": "Edit question attributes in bulk.",
  "subquestionDetails": "subquestion details",
  "subquestions": "subquestions",
  "binaryQuestionGroup": "binary question group",
  "binaryQuestionGroupDescription": "Binary question groups contain questions that generally have the form \"Will X happen?\" and resolve as Yes or No.",
  "numericQuestionGroup": "numeric question group",
  "numericQuestionGroupDescription": "Numeric question groups contain questions that ask about the value of an unknown future quantity and resolve within a specified range.",
  "dateQuestionGroup": "date question group",
  "dateQuestionGroupDescription": "Date question groups contain questions that ask when something will happen and resolve within a specified date range.",
  "groupVariableDescription": "What the subquestion labels represent (e.g. year, country, etc).",
  "categoryPickerDescription": "A name for the parameter which varies between subquestions, like \"Option\", \"Year\" or \"Country\"",
  "subquestionLabel": "Subquestion label",
  "subquestionLabelDescription": "The label or parameter which identifies this subquestion, like \"Option 1\", \"2033\" or \"France\"",
  "conditionalPairDescription": "A Conditional Pair is a special type of Question Group that elicits conditional probabilities. Each Conditional Pair sits between a Parent Question and a Child Question.  The Parent must be a binary question, while the child may be a single binary, numeric, or date question.",
  "parentId": "Parent ID",
  "parentInputDescription": "Please enter the URL/ID of a binary question. If selecting a subquestion, please enter the full url e.g. /questions/123/?sub-question=456",
  "childId": "Child ID",
  "childInputDescription": "Please enter the URL/ID of a binary, numeric, or date question. Multiple Choice coming soon. If selecting a subquestion, please enter the full url e.g. /questions/123/?sub-question=456",
  "enterQuestionIdUrl": "Please enter the URL/ID of an existing Metaculus question.",
  "isDoneError": "Cannot edit closed, resolved or rejected questions",
  "editNotebook": "edit notebook",
  "createNotebook": "create notebook",
  "metaculusOnTwitter": "Metaculus on Twitter",
  "metaculusOnDiscord": "Metaculus on Discord",
  "signUpAsBot": "Sign Up as Bot",
  "failedToCopyText": "failed to copy text: ",
  "resolutionLabel": "Resolution:",
  "totalQuestions": "Total questions:",
  "averageScore": "Average score:",
  "medianScore": "Median Score:",
  "communityPredictionCalibration": "Community Prediction calibration",
  "communityPredictionBaselineScore": "Community Prediction Baseline score",
  "confidenceInterval": "confidence interval",
  "perfectCalibration": "perfect calibration",
  "perfectCalibration90CI": "perfect calibration 90% CI",
  "userCalibration": "{username}'s calibration",
  "userPeerScore": "{username}'s Peer Score",
  "userPrediction": "{username}'s prediction",
  "fractionResolvedYes": "Fraction resolved yes",
  "scatterPlotHoverMessage": "Hover over a circle to see how that question resolved.",
  "calibrationCurve": "Calibration Curve",
  "calibrationCurveInfoMain": "The Calibration Curve only takes into account forecasts for binary questions that have resolved in the last 5 years and whose scheduled resolution date has passed. The x-axis shows prediction ranges (e.g. 17.5% - 22.5%) and the y-axis shows the actual \"Yes\" resolution rate for within each range. Yellow diamonds represent these observed rates. The gray lines along the diagonal (y=x) are what you would get by forecasting the \"true\" probability on a large number of questions. The shaded area is the 90% credible interval around the perfect calibration, the width of which depends on how many predictions the forecaster has made in that x-axis range. If the forecaster had been perfectly calibrated, the diamond would have been in that range 90% of the time.",
  "calibrationCurveInfo": "The closer a diamond is to the grey line, the better calibrated the predictions in that probability range. If a diamond is closer to y=50% than the grey bar, that suggests the forecaster is overconfident.",
  "condition": "condition",
  "facebook": "Facebook",
  "xTwitter": "X / Twitter",
  "copyFromBranch": "copy from {branch}",
  "parentResolvesAsYes": "Parent Resolves as Yes",
  "parentResolvesAsNo": "Parent Resolves as No",
  "othersCount": "{count} others",
  "selectAll": "select all",
  "deselectAll": "deselect all",
  "forecastDataIsEmpty": "Forecast data is empty",
  "noForecastsYet": "No forecasts yet",
  "RevealTemporarily": "Reveal Temporarily",
  "CPIsHidden": "Community Prediction is hidden",
  "createdByUserOnDate": "Created by {user} on {date}",
  "edited": "edited",
  "tournamentHidePostsToMainFeed": "Hide tournament posts in main feed",
  "me": "me",
  "forecastDisclaimer": "Based on {predictionCount} predictions by {forecasterCount} forecasters",
  "embedCodeSnippet": "You can use the below code snippet to embed this page on your own webpage. Feel free to change the height and width to suit your needs.",
  "embedThisPage": "embed this page",
  "selectATheme": "Select a theme",
  "selectAGraphZoom": "Select a graph zoom",
  "light": "light",
  "dark": "dark",
  "all": "all",
  "2m": "2m",
  "1w": "1w",
  "1d": "1d",
  "errorDeletingComment": "error deleting comment:",
  "numeric": "numeric",
  "date": "date",
  "filterBy": "filter by",
  "closeFilter": "close filter",
  "biosecurity": "biosecurity",
  "biosecurityDescription": "Improving global health by understanding infectious diseases and preparing for future pandemics",
  "aiProgress": "AI progress",
  "aiProgressDescription": "Exploring the future of artificial intelligence technologies and the impacts to society",
  "nuclearSecurity": "nuclear security",
  "nuclearSecurityDescription": "Quantifying global risks to keep us safe and secure for a flourishing future",
  "climateChange": "climate change",
  "climateChangeDescription": "Predicting long-term shifts in temperature and weather patterns caused by human activity",
  "cookiesTitle": "We use cookies \uD83C\uDF6A to understand how you use Metaculus and to improve your experience.",
  "cookiesSubtitle": "Learn more about how we use cookies in our <link>Privacy Policy</link>",
  "customize": "Customize",
  "acceptAndClose": "Accept and close",
  "cookiePreferences": "Cookie Preferences",
  "cookieServices": "Metaculus uses cookies for the following types of services.",
  "learnMoreFromPrivacyPolicy": "Learn more from our <link>Privacy Policy</link>.",
  "necessaryOnly": "Necessary only",
  "necessaryCookies": "Necessary cookies",
  "cannotBeUnchecked": "Cannot be unchecked",
  "analytics": "Analytics",
  "helpsImprovePlatform": "Helps us improve the platform",
  "saveSelected": "Save Selected",
  "onboardingStep1Question1": "Should I bring an umbrella?",
  "onboardingStep1Question2": "Are we headed for a recession?",
  "onboardingStep1Question3": "Will my team come out on top?",
  "onboardingStep1Paragraph1": "You make predictions all the time, on topics big and small. It's a skill, and Metaculus helps you hone it.",
  "onboardingStep1Paragraph2": "Let's make a few quick predictions to get you started.",
  "onboardingStep1Paragraph3": "First, pick a topic you care about.",
  "skipTutorial": "Skip Tutorial",
  "remindMeLater": "Remind me later",
  "onboardingRemindMeLaterDescription": "(You can always find the tutorial later in the menu at the top of every page)",
  "skipQuestions": "Skip question",
  "onboardingStep2Title": "Here's a real Metaculus question about {topicName}:",
  "onboardingStep2CommunityThinks": "Other forecasters tend to think this is",
  "onboardingStep2CommunityGives": "They give it",
  "onboardingStep2WhatDoYouThink": "What do you think? Do you agree with",
  "onboardingStep2Disagree": "Disagree?",
  "onboardingStep2LessLikely": "Less likely",
  "onboardingStep2AboutRight": "About right",
  "onboardingStep2MoreLikely": "More likely",
  "onboardingStep2Excellent": "Excellent. Below you can see your prediction quantified next to the community's. Does that look about right? If not, you can grab and drag the slider. Click 'Predict' when you're ready to continue.",
  "onboardingStep2Predict": "Predict",
  "onboardingStep2Error": "An error occurred while submitting your forecast.",
  "onboardingStep3Title": "Great! Here's another question about {topicName}:",
  "onboardingStep3CommunityThinks": "Other forecasters tend to think this is",
  "onboardingStep3CommunityGives": "They give it",
  "onboardingStep3WhatDoYouThink": "How likely do you think this is?",
  "onboardingStep3Predict": "Predict",
  "onboardingStep4Factors": "Here are some of the factors other forecasters are considering.",
  "onboardingStep4ConsideringOthers": "Considering others' views is an important step in forecasting accurately!",
  "onboardingStep4WhatDoYouThink": "What do you think? Did any of those factors make you want to change your prediction? If not, that's fine too.",
  "onboardingStep4AlmostDone": "You are almost done with this tutorial!",
  "onboardingStep4Predict": "Predict",
  "onboardingStep4Error": "An error occurred while submitting your forecast.",
  "onboardingStep5NiceWork": "Nice work!",
  "onboardingStep5WellDone": "Well done!",
  "onboardingStep5SavePredictionsParagraph": "Would you like to save the predictions you've made?",
  "onboardingYouPredicted": "You predicted",
  "onboardingStep5AnyoneCanImprove": "Anyone can improve at forecasting by practicing and thinking through what factors could influence outcomes.",
  "onboardingStep5DidYouKnow": "Did you know?",
  "onboardingStep5ForecastingCompetition": "In a series of forecasting competitions conducted by University of Pennsylvania professor Philip Tetlock, skilled forecasters outperformed CIA analysts without access to classified intelligence.",
  "onboardingStep5ReadyToExplore": "You are now ready to explore Metaculus by yourself. What would you like to do next?",
  "onboardingStep5ViewYourPredictions": "View Your Predictions",
  "onboardingStep5ForecastAnother": "Forecast Another",
  "onboardingStep5Question": "Question",
  "onboardingStep5ViewQuestionFeed": "View Question Feed",
  "communityName": "Community Name",
  "communityNameDescription": "This is how your community appears to others.",
  "communitySlug": "Community Slug",
  "communitySlugDescription": "Your community can be accessed via the following URL:",
  "communityDescription": "Community Description",
  "youArePostingAPrivateComment": "You are posting a private comment",
  "unread": "unread",
  "contentTranslatedHeaderText": "Some content on this page is automatically translated, and may be inaccurate.",
  "showOriginalContent": "Show original",
  "willForecastingOn": "You’ll be forecasting on {questionNumber} questions",
  "authOnMetaculus": "Authenticate on Metaculus",
  "questionsToForecast": "There are <bold>{questionNumber} questions</bold> to forecast.",
  "forecastedOn": "You forecasted on <bold>{forecastedNumber}</bold> of <bold>{questionNumber}</bold> available questions",
  "startForecasting": "Start forecasting",
  "continueForecasting": "Continue forecasting",
  "curveIntroduction": "Forecast to help build the AGI readiness survey, and see where you fall relative to the Curve Crowd",
  "curveHistogramHeader": "Check out how your answers compare to the rest of the group. When you’re ready, click Next Question.",
  "nextQuestion": "Next Question",
  "next": "Next",
  "curveSurvey": "AGI Readiness Survey",
  "viewTournamentPage": "View Tournament Page",
  "joinAgiWorkshop": "Join the AGI Readiness Workshop on Saturday at 4pm to generate and discuss more questions.",
  "visitTournamentPage": "If you’d like to change your forecasts, visit the Tournament Page.",
  "forecastedAllQuestions": "You’ve forecasted on all {questionNumber} questions!",
  "answerAfterEditing": "Answers cannot be edited after submission.",
  "fullName": "Full Name",
  "country": "Country of Residence",
  "undergraduateStudentQuestion": "Are you currently an undergraduate student?",
  "institution": "Institution",
  "major": "Major",
  "graduationYear": "Expected Graduation Year",
  "finishRegistration": "Finish Registration",
  "keyFactor": "Key Factor",
  "keyFactors": "Key Factors",
  "notAvailable": "Not available",
  "indexesTitle": "Project",
  "indexScore": "Index value: <bold>{value}</bold>",
  "indexQuestion": "Question",
  "indexWeight": "Weight",
  "indexCP": "CP",
  "weeklyMovementChange": "{value} this week",
  "percentagePoints": "percentage points",
  "proForecastersTitle": "About Metaculus Pro Forecasters",
  "proForecastersDescription": "For certain projects Metaculus employs Pro Forecasters who have demonstrated excellent forecasting ability and who have a history of clearly describing their rationales. Pros forecast on private and public sets of questions to produce well-calibrated forecasts and descriptive rationales for our partners.<br></br><br></br>If you’re interested in hiring Metaculus Pro Forecasters for a project, contact us at <email>support@metaculus.com</email> with the subject <strong>\"Project Inquiry\"</strong>.",
  "meetProForecasters": "Meet a few of our Pro Forecasters:",
  "whyUseProForecastersTitle": "Why Use Pro Forecasters?",
  "whyUseProForecastersInfo": "The Metaculus community has demonstrated <link>excellent accuracy and calibration</link> on the questions it has forecasted. However, sometimes there are questions of particular importance – either to the world or to certain stakeholders – where excellent calibration and transparent reasoning are of the utmost importance. Pro Forecasters are selected from the most accurate Metaculus forecasters and describe their reasoning when making forecasts, providing enhanced confidence in the calibration of the predictions and ensuring that stakeholders can understand what is driving the forecasts.",
  "selectingProForecastersTitle": "Selecting Pro Forecasters",
  "selectingProForecastersInfo": "Pro Forecasters are carefully selected from forecasters who have fulfilled certain criteria to ensure the quality of their predictions and reasoning.",
  "excellentForecastAbilityTitle": "Excellent Forecasting Ability",
  "excellentForecastAbilityInfo": "Pro Forecasters must demonstrate excellent forecasting ability. This means not only that they have excellent accuracy, but also that they have a history of making insightful comments that clearly explain their reasoning. Accuracy alone is not enough, a Pro Forecaster must be able to both make accurate forecasts and explain the underlying reasoning behind those forecasts.<br></br><br></br>Our Pro selection methodology uses the <link>Metaculus Leaderboards</link>, combining the Peer Accuracy, Baseline Accuracy, and Comments leaderboards to produce a weighted average score across those leaderboards and across different leaderboard periods. Pros are selected from forecasters who have the highest score on this combined metric, representing the very best forecasters from all of Metaculus. Note that while the Peer score is weighted highest in this combined metric, the weighting is such that forecasters must have good scores in all categories.",
  "robustTrackRecordsTitle": "Robust Track Records",
  "robustTrackRecordsInfo": "Pro Forecasters must have at least 75 resolved questions and must have made predictions across multiple subject areas, with at least one year of experience making predictions.<br></br><br></br>We also consider recruiting forecasters who have demonstrated excellent forecasting ability elsewhere.",
  "clearCommentsAndCommunicationTitle": "Clear Comments and Communication",
  "clearCommentsAndCommunicationInfo": "Our Pros work on projects for external partners who value clear reasoning to better interpret the forecasts. We select Pros who have a history of making clear and insightful comments, and who are willing to disagree with their peers, but in a polite and respectful manner.",
  "expressionOfInterestFormMessage": "We sometimes recruit upstanding members of the community who are great at providing constructive feedback on submitted questions to become paid moderators. Fill out our <link>expression of interest form</link> if you would like to be considered.",
  "dateInputDetails": "Date and time is in your local timezone [{timezone}]",
  "postCreateErrorMinSubquestions": "A question group must have at least one subquestion",
  "insertEquation": "Insert Equation",
  "choseEquationType": "Choose the type of equation you want to add. To edit it, simply click on it within the editor.",
  "inline": "Inline",
  "block": "Block",
  "accountInactiveModalTitle": "You've got mail!",
  "accountInactiveModalBody": "Please confirm your email associated with your <bold>{login}</bold> account to be able to log in.",
  "accountInactiveModalResendButton": "Resend activation email",

  "faq_MetaculusFAQ": "Metaculus FAQ",
  "faq__content_1": "Basics",
  "faq__content_2": "What is Metaculus?",
  "faq__content_3": "What is forecasting?",
  "faq__content_4": "When is forecasting valuable?",
  "faq__content_5": "Why should I be a forecaster?",
  "faq__content_6": "Who created Metaculus?",
  "faq__content_7": "What are Metaculus Tournaments and Question Series?",
  "faq__content_8": "Is Metaculus a prediction market?",
  "faq__content_9": "Are Metaculus questions Polls?",
  "faq__content_10": "Metaculus Questions",
  "faq__content_11": "What sorts of questions are allowed, and what makes a good question?",
  "faq__content_12": "Who creates the questions, and who decides which get posted?",
  "faq__content_13": "Who can edit questions?",
  "faq__content_14": "How can I get my own question posted?",
  "faq__content_15": "What can I do if a question I submitted has been pending for a long time?",
  "faq__content_16": "What can I do if a question should be resolved but isn't?",
  "faq__content_17": "What is a private question?",
  "faq__content_18": "What are the rules and guidelines for comments and discussions?",
  "faq__content_19": "What do \"credible source\" and \"before [date X]\" and such phrases mean exactly?",
  "faq__content_20": "What types of questions are there?",
  "faq__content_21": "What are question groups?",
  "faq__content_22": "What are Conditional Pairs?",
  "faq__content_23": "How do I find certain questions on Metaculus?",
  "faq__content_24": "Question Resolution",
  "faq__content_25": "What are the \"open date\", \"close date\" and \"resolve date?\"",
  "faq__content_26": "What timezone is used for questions?",
  "faq__content_27": "Who decides the resolution to a question?",
  "faq__content_28": "What are \"Ambiguous\" and \"Annulled\" resolutions?",
  "faq__content_29": "Do all questions get resolved?",
  "faq__content_30": "When will a question be resolved?",
  "faq__content_31": "Is the background material used for question resolution?",
  "faq__content_32": "What happens if the resolution criteria of a question is unclear or suboptimal?",
  "faq__content_33": "Can questions be re-resolved?",
  "faq__content_34": "What happens if a question gets resolved in the real world prior to the close time?",
  "faq__content_35": "When should a question specify retroactive closure?",
  "faq__content_36": "What happens if a question's resolution criteria turn out to have been fulfilled prior to the opening time?",
  "faq__content_37": "What happens if a resolution source is no longer available?",
  "faq__content_38": "What are Resolution Councils?",
  "faq__content_39": "Predictions",
  "faq__content_40": "Is there a tutorial or walkthrough?",
  "faq__content_41": "How do I make a prediction? Can I change it later?",
  "faq__content_42": "How can I withdraw my prediction?",
  "faq__content_43": "How do I use the range interface?",
  "faq__content_44": "How is the Community Prediction calculated?",
  "faq__content_45": "What is the Metaculus Prediction?",
  "faq__content_46": "What are public figure predictions?",
  "faq__content_47": "What is \"Reaffirming\" a prediction?",
  "faq__content_48": "Scores and Medals",
  "faq__content_49": "What are scores?",
  "faq__content_50": "What are medals?",
  "faq__content_51": "Metaculus Journal",
  "faq__content_52": "What is the metaculus Journal?",
  "faq__content_53": "What is a fortified essay?",
  "faq__content_54": "Miscellany",
  "faq__content_55": "What are Metaculus Pro Forecasters?",
  "faq__content_56": "Does Metaculus have an API?",
  "faq__content_57": "How do I change my username?",
  "faq__content_58": "I've registered an account. Why can't I comment on a question?",
  "faq__content_59": "Understanding account suspensions",
  "faq__content_60": "Why can I see the Community Prediction on some questions, the Metaculus Prediction on others, and no prediction on some others?",
  "faq__content_61": "What is NewsMatch?",
  "faq__content_62": "What are Community Insights?",
  "faq__content_63": "Can I get my own Metaculus?",
  "faq__content_64": "How can I help spread the word about Metaculus?",
  "faq__content_65": "How can I close my account and delete my personal information on Metaculus?",
  "faq_Basics": "Basics",
  "faq_WhatisMetaculus?": "What is Metaculus?",
  "faq_whatismetaculus_content_1": "Metaculus is an online forecasting platform and aggregation engine that brings together a global reasoning community and keeps score for thousands of forecasters, delivering machine learning-optimized aggregate forecasts on topics of global importance. The Metaculus forecasting community is often inspired by altruistic causes, and Metaculus has a long history of partnering with nonprofit organizations, university researchers and companies to increase the positive impact of its forecasts.",
  "faq_whatismetaculus_content_2": "Metaculus therefore poses questions about the occurrence of a variety of future events, on many timescales, to a community of participating forecasters — you!",
  "faq_whatismetaculus_content_3": "The name \"Metaculus\" comes from the ",
  "faq_whatismetaculus_content_4": "Metaculus genus",
  "faq_whatismetaculus_content_5": " in the Eriophyidae family, a genus of herbivorous mites found in many locations around the world.",
  "faq_Whatisforecasting?": "What is forecasting?",
  "faq_whatisforecasting_content_1": "Forecasting is a systematic practice of attempting to answer questions about future events. On Metaculus, we follow a few principles to elevate forecasting above simple guesswork:",
  "faq_whatisforecasting_content_2": "First, questions are carefully specified so that everyone understands beforehand and afterward which kinds of outcomes are included in the resolution, and which are not. Forecasters then give precise probabilities that measure their uncertainty about the outcome.",
  "faq_whatisforecasting_content_3": "Second, Metaculus aggregates the forecasts into a community prediction based on the",
  "faq_whatisforecasting_content_4": "median",
  "faq_whatisforecasting_content_5": "of user forecasts weighted by recency. Surprisingly, the Community Prediction is often ",
  "faq_whatisforecasting_content_6": "better than any individual predictor",
  "faq_whatisforecasting_content_7": "! This principle is known as ",
  "faq_whatisforecasting_content_8": "the wisdom of the crowd,",
  "faq_whatisforecasting_content_9": " and has been demonstrated on Metaculus and by other researchers. Intuitively it makes sense, as each individual has separate information and biases which in general balance each other out (provided the whole group is not biased in the same way).",
  "faq_whatisforecasting_content_10": "Third, we measure the relative skill of each forecaster, using their quantified forecasts. When we know the outcome of the question, the question is \"resolved\", and forecasters receive their scores. By tracking these scores from many forecasts on different topics over a long period of time, they become an increasingly better metric of how good a given forecaster is. These scores provide aspiring forecasters with important feedback on how they did and where they can improve.",
  "faq_Whenisforecastingvaluable?": "When is forecasting valuable?",
  "faq_whenforecastingvaluable_content_1": "Forecasting is uniquely valuable primarily in complex, multi-variable problems or in situations where a lack of data makes it difficult to predict using explicit or exact models.",
  "faq_whenforecastingvaluable_content_2": "In these and other scenarios, aggregated predictions of strong forecasters offer one of the best ways of predicting future events. In fact, work by the political scientist Philip Tetlock demonstrated that aggregated predictions were able to outperform professional intelligence analysts with access to classified information when forecasting various geopolitical outcomes.",
  "faq_WhyshouldIbeaforecaster?": "Why should I be a forecaster?",
  "faq_aim_content_1": "Research has shown that great forecasters come from various backgrounds—and oftentimes from fields that have nothing to do with predicting the future. Like many mental capabilities, prediction is a talent that persists over time and is a skill that can be developed. Steady quantitative feedback and regular practice can greatly improve a forecaster's accuracy.",
  "faq_aim_content_2": "Some events — such as eclipse timing and well-polled elections, can often be predicted with high resolution, e.g. 99.9% likely or 3% likely. Others — such as the flip of a coin or a close horse-race — cannot be accurately predicted; but their odds still can be. Metaculus aims at both: to provide a central generation and aggregation point for predictions. With these in hand, we believe that individuals, groups, corporations, governments, and humanity as a whole will make better decisions.",
  "faq_aim_content_3": "As well as being worthwhile, Metaculus aims to be interesting and fun, while allowing participants to hone their prediction prowess and amass a track-record to prove it.",
  "faq_WhocreatedMetaculus?": "Who created Metaculus?",
  "faq_whocreated_content_1": " Metaculus originated with two researcher scientists, Anthony Aguirre and Greg Laughlin. Aguirre, a physicist, is a co-founder of ",
  "faq_whocreated_content_2": "The Foundational Questions Institute",
  "faq_whocreated_content_3": ", which catalyzes breakthrough research in fundamental physics, and of ",
  "faq_whocreated_content_4": "The Future of Life Institute",
  "faq_whocreated_content_5": ", which aims to increase the benefit and safety of disruptive technologies like AI. Laughlin, an astrophysicist, is an expert at predictions from the millisecond predictions relevant to high-frequency trading to the ultra-long-term stability of the solar system.",
  "faq_WhatAreMetaculusTournamentsandQuestionSeries?": "What Are Metaculus Tournaments and Question Series?",
  "faq_whattournaments_content_1": "Tournaments",
  "faq_whattournaments_content_2": "Metaculus tournaments are organized around a central topic or theme. Tournaments are often collaborations between Metaculus and a nonprofit, government agency, or other organization seeking to use forecasting to support effective decision making. You can find current and archived tournaments in our ",
  "faq_whattournaments_content_3": "Tournaments page",
  "faq_whattournaments_content_4": "Tournaments are the perfect place to prove your forecasting skills, while helping to improve our collective decision making ability. Cash prizes and",
  "faq_whattournaments_content_5": "Medals",
  "faq_whattournaments_content_6": "are awarded to the most accurate forecasters, and sometimes for other valuable contributions (like comments). Follow a Tournament (with the Follow button) to never miss new questions.",
  "faq_whattournaments_content_7": "After at least one question has resolved, a Leaderboard will appear on the tournament page displaying current scores and rankings. A personal score board (\"My Score\") will also appear, detailing your performance for each question (see ",
  "faq_whattournaments_content_8": "How are Tournaments Scored?",
  "faq_whattournaments_content_9": "At the end of a tournament, the prize pool is divided among forecasters according to their forecasting performance. The more you forecasted and the more accurate your forecasts were, the greater proportion of the prize pool you receive.",
  "faq_whattournaments_content_10": "Can I donate my tournament winnings?",
  "faq_whattournaments_content_11": "If you have outstanding tournament winnings, Metaculus is happy to facilitate donations to various non-profits, regranting organizations, and funds. You can find the list of organizations we facilitate payments to ",
  "faq_whattournaments_content_12": "here",
  "faq_whattournaments_content_13": "Question Series",
  "faq_whattournaments_content_14": "Like Tournaments, Question Series are organized around a central topic or theme. Unlike tournaments, they do not have a prize pool.",
  "faq_whattournaments_content_15": "Question Series still show leaderboards, for interest and fun. However they do",
  "faq_whattournaments_content_16": "not",
  "faq_whattournaments_content_17": "award medals.",
  "faq_whattournaments_content_18": "You can find all Question Series in a special section of the ",
  "faq_whattournaments_content_19": "Tournaments page",
  "faq_IsMetaculusapredictionmarket?": "Is Metaculus a prediction market?",
  "faq_predmarket_content_1": "Metaculus has some similarities to a prediction market, but ultimately is not one. Metaculus aims to aggregate many people's information, expertise, and predictive power into high-quality forecasts. However, prediction markets generally operate using real or virtual currency, where participants buy (or sell) shares if they think that the standing prices reflect too low (or high) a probability of an event occurring. Metaculus, in contrast, directly solicits predicted probabilities from its users, then aggregates those probabilities. We believe that this sort of \"prediction aggregator\" has both advantages and disadvantages relative to a prediction market.",
  "faq_predmarket_content_2": "Advantages of Metaculus over prediction markets",
  "faq_predmarket_content_3": "Metaculus has several advantages over prediction markets, described below, but we want to preface this by saying that despite the potential issues with prediction markets that we describe here, we think prediction markets are valuable, are glad they exist, and would be glad to see more use of them.",
  "faq_predmarket_content_4": "Poor incentives for longer term forecasts.",
  "faq_predmarket_content_5": "It's usually not a good use of your funds to lock them up in a prediction market for the long term, since you can usually get much better returns by investing, which means longer term markets are likely to have low liquidity. For an example see ",
  "faq_predmarket_content_6": "this plot",
  "faq_predmarket_content_7": " from a ",
  "faq_predmarket_content_8": "Works in Progress article",
  "faq_predmarket_content_9": " showing the trading volume on Betfair for the 2020 US presidential election. There was very little volume far in advance of the election, with most of the trading volume occurring only a month out from the election.",
  "faq_predmarket_content_10": "Problems with low probabilities.",
  "faq_predmarket_content_11": "Prediction markets have market frictions that make them less useful for low probabilities. The return on using your money to bring a probability from 2% to 1% is negligible, or potentially negative if the prediction market extract a fee from traders. That's why you get weird results like Michelle Obama at 6% chance of becoming the Democratic nominee for the 2024 US presidential election in June of 2024, as was the case ",
  "faq_predmarket_content_12": "on Polymarket",
  "faq_predmarket_content_13": "The focus isn't always forecasting.",
  "faq_predmarket_content_14": "Prediction market incentives aren't always aligned with making the most accurate predictions. Consider that one potential use for prediction markets is hedging against risky outcomes. Additionally, people who are irrational but willing to put a ton of money behind their beliefs may skew the outcome. Sure, ideally a liquid market will correct for these skews, but it's possible that they could have an effect on the price. See ",
  "faq_predmarket_content_15": "this piece in Asterisk Magazine",
  "faq_predmarket_content_16": " for more on \"dumb money\" in prediction markets.",
  "faq_predmarket_content_17": "What do individuals think will happen?",
  "faq_predmarket_content_18": "Participants in prediction markets are expressing whether they think the probability is higher or lower than the market price, not making a forecast. If someone thinks the market is too low at 35% and bets accordingly, you don't know whether they think the true probability is 40% or 80%. This doesn't really impact the usefulness of the aggregate, but it does make the data less rich and informative, and harder to see the full distribution of forecasts like you can with the histograms for binary questions on Metaculus.",
  "faq_predmarket_content_19": "Individual market performance is not always a clear indication of forecasting skill.",
  "faq_predmarket_content_20": " Excellent individual market performance might just signal proficiency at operating in markets, or ability to take advantage of bad bets made by others. For example, see ",
  "faq_predmarket_content_21": "this post",
  "faq_predmarket_content_22": " about a tournament organized on Manifold where traders took a large early lead just due to intelligent use of limit orders. Since Metaculus elicits individual probabilities from every forecaster, we can better assess and recruit excellent forecasters.",
  "faq_predmarket_content_23": "Metaculus performs comparably to markets without the need to manage a portfolio.",
  "faq_predmarket_content_24": " There are only a handful of apples-to-apples comparisons between platforms, but ",
  "faq_predmarket_content_25": "these",
  "faq_predmarket_content_26": "find",
  "faq_predmarket_content_27": " an ",
  "faq_predmarket_content_28": "advantage",
  "faq_predmarket_content_29": " for Metaculus over prediction markets. Note that sample sizes tend to be small. However, there is also an ",
  "faq_predmarket_content_30": "indirect comparison",
  "faq_predmarket_content_31": " (indirect because it does not consider the same questions across platforms) which found that prediction markets are more calibrated.",
  "faq_AreMetaculusQuestionsPolls?": "Are Metaculus Questions Polls?",
  "faq_justpolling_content_1": "No. Opinion polling can be a useful way to gauge the sentiment and changes in a group or culture, but there is often no single \"right answer\", as in a ",
  "faq_justpolling_content_2": "Gallup poll",
  "faq_justpolling_content_3": " \"How worried are you about the environment?\"",
  "faq_justpolling_content_4": "In contrast, Metaculus questions are designed to be objectively resolvable (like in ",
  "faq_justpolling_content_5": "Will Brent Crude Oil top $140/barrel before May 2022?",
  "faq_justpolling_content_6": "), and forecasters are not asked for their preferences, but for their predictions. Unlike in a poll, over many predictions, participants accrue a track record indicating their forecasting accuracy. These track records are incorporated into the ",
  "faq_justpolling_content_7": "Metaculus Prediction",
  "faq_justpolling_content_8": ". The accuracy of the Metaculus track record itself is tracked ",
  "faq_justpolling_content_9": "here",
  "faq_MetaculusQuestions": "Metaculus Questions",
  "faq_Whatsortsofquestionsareallowed,andwhatmakesagoodquestion?": "What sorts of questions are allowed, and what makes a good question?",
  "faq_whatsort_content_1": "Questions should focus on tangible, objective facts about the world which are well-defined and not a matter of opinion. \"When will the United States collapse?\" is a poor, ambiguous question; ",
  "faq_whatsort_content_2": "What will be the US' score in the Freedom in the World Report for 2050?",
  "faq_whatsort_content_3": " is more clear and definite. They generally take the form ",
  "faq_whatsort_content_4": "Will (event) X happen by (date) Y?",
  "faq_whatsort_content_5": "or ",
  "faq_whatsort_content_6": "When will (event) X occur?",
  "faq_whatsort_content_7": "or ",
  "faq_whatsort_content_8": "What will the value or quantity of X be by (date) Y?",
  "faq_whatsort_content_9": "A good question will be unambiguously resolvable. A community reading the question terms should be able to agree, before and after the event has occurred, whether the outcome satisfies the question's terms.",
  "faq_whatsort_content_10": "Questions should also follow some obvious rules:",
  "faq_whatsort_content_11": "Questions should respect privacy and not address the personal lives of non-public figures.",
  "faq_whatsort_content_12": "Questions should not be directly potentially defamatory or generally in bad taste.",
  "faq_whatsort_content_13": "Questions should never aim to predict mortality of individual people or even small groups. In cases of public interest (such as court appointees and political figures), the question should be phrased in other more directly relevant terms such as \"when will X no longer serve on the court\" or \"will Y be unable to run for office on date X\". When the topic is death (or longevity) itself questions should treat people in aggregate or hypothetically.",
  "faq_whatsort_content_14": "More generally, questions should avoid being written in a way that incentivizes illegal or harmful acts — that is, hypothetically, if someone were motivated enough by a Metaculus Question to influence the real world and change the outcome of a question's resolution, those actions should not be inherently illegal or harmful.",
  "faq_Whocreatesthequestions,andwhodecideswhichgetposted?": "Who creates the questions, and who decides which get posted?",
  "faq_whocreates_content_1": "Many questions are launched by Metaculus staff, but any logged-in user can propose a question. Proposed questions will be reviewed by a group of moderators appointed by Metaculus. Moderators will select the best questions submitted, and help to edit the question to be clear, well-sourced, and ",
  "faq_whocreates_content_2": "aligned with our writing style",
  "faq_whocreates_content_3": "Metaculus hosts questions on ",
  "faq_whocreates_content_4": "many topics",
  "faq_whocreates_content_5": ", but our primary focus areas are Science, ",
  "faq_whocreates_content_6": "Technology",
  "faq_whocreates_content_7": "Effective Altruism",
  "faq_whocreates_content_8": "Artificial Intelligence",
  "faq_whocreates_content_9": "Health",
  "faq_whocreates_content_10": ", and ",
  "faq_whocreates_content_11": "Geopolitics",
  "faq_Whocaneditquestions?": "Who can edit questions?",
  "faq_whoedits_content_1": "Admins can edit all questions at any time (however, once predictions have begun, great care is taken not to change a question's resolution terms unless necessary).",
  "faq_whoedits_content_2": "Moderators can edit questions when they are Pending and Upcoming (before predictions have begun).",
  "faq_whoedits_content_3": "Authors can edit their questions when they are Drafts and Pending.",
  "faq_whoedits_content_4": "Authors can invite other users to edit questions that are in Draft or Pending.",
  "faq_HowcanIgetmyownquestionposted?": "How can I get my own question posted?",
  "faq_question-submission_content_1": "If you have a basic idea for a question but don't have time/energy to work out the details, you're welcome to submit it, discuss it in our ",
  "faq_question-submission_content_2": "question idea thread",
  "faq_question-submission_content_3": ", or on our ",
  "faq_question-submission_content_4": "Discord channel",
  "faq_question-submission_content_5": "If you have a pretty fully-formed question, with at least a couple of linked references and fairly careful unambiguous resolution criteria, it's likely that your question will be reviewed and launched quickly.",
  "faq_question-submission_content_6": "Metaculus hosts questions on ",
  "faq_question-submission_content_7": "many topics",
  "faq_question-submission_content_8": ", but our primary focus areas are Science, ",
  "faq_question-submission_content_9": "Technology",
  "faq_question-submission_content_10": "Effective Altruism",
  "faq_question-submission_content_11": "Artificial Intelligence",
  "faq_question-submission_content_12": "Health",
  "faq_question-submission_content_13": ", and ",
  "faq_question-submission_content_14": "Geopolitics",
  "faq_question-submission_content_15": ". Questions on other topics, especially that require a lot of moderator effort to get launched, will be given lower priority and may be deferred until a later time.",
  "faq_question-submission_content_16": "We regard submitted questions as suggestions and take a free hand in editing them. If you're worried about having your name on a question that is altered from what you submit, or would like to see the question before it's launched, please note this in the question itself; questions are hidden from public view until they are given \"upcoming\" status, and can be posted anonymously upon request.",
  "faq_WhatcanIdoifaquestionIsubmittedhasbeenpendingforalongtime?": "What can I do if a question I submitted has been pending for a long time?",
  "faq_pending-question_content_1": "We currently receive a large volume of question submissions, many of which are interesting and well-written. That said, we try to approve just enough questions that they each can get the attention they deserve from our forecasters. Metaculus prioritizes questions on Science,",
  "faq_pending-question_content_2": "Technology",
  "faq_pending-question_content_3": "Effective Altruism",
  "faq_pending-question_content_4": "Artificial Intelligence",
  "faq_pending-question_content_5": "Health",
  "faq_pending-question_content_6": ", and ",
  "faq_pending-question_content_7": "Geopolitics",
  "faq_pending-question_content_8": ". If your question falls into one of these categories, or is otherwise very urgent or important, you can tag us with @moderators to get our attention.",
  "faq_WhatcanIdoifaquestionshouldberesolvedbutisnt?": "What can I do if a question should be resolved but isn't?",
  "faq_admins-resolution_content_1": "If a question is still waiting for resolution, check to make sure there hasn't been a comment from staff explaining the reason for the delay. If there hasn't, you can tag @admins to alert the Metaculus team. Please do not use the @admins tag more than once per week regarding a single question or resolution.",
  "faq_Whatisaprivatequestion?": "What is a private question?",
  "faq_question-private_content_1": "Private questions are questions that are not visible to the broader community. They aren't subject to the normal review process, so you can create one and predict on it right away. You can resolve your own private questions at any time, but points for private predictions won't be added to your overall Metaculus score and they won't affect your ranking on the leaderboard.",
  "faq_question-private_content_2": "You can use private questions for anything you want. Use them as practice to calibrate your predictions before playing for points, create a question series on a niche topic, or pose personal questions that only you can resolve. ",
  "faq_question-private_content_3": "You can even invite up to 19 other users",
  "faq_question-private_content_4": "to view and predict on your own questions!",
  "faq_question-private_content_5": "To invite other forecasters to your private question, click the '...' more options menu and select 'Share Private Question'.",
  "faq_Whataretherulesandguidelinesforcommentsanddiscussions?": "What are the rules and guidelines for comments and discussions?",
  "faq_comments_content_1": "We have a full set of ",
  "faq_comments_content_2": "community etiquette guidelines",
  "faq_comments_content_3": "but in summary:",
  "faq_comments_content_4": "Users are welcome comment on any question.",
  "faq_comments_content_5": "Comments and questions can use ",
  "faq_comments_content_6": "markdown formatting",
  "faq_comments_content_7": "Metaculus aims at a high level of discourse. Comments should be on topic, relevant and interesting. Comments should not only state the author's opinion (with the exception of quantified predictions). Comments which are spammy, aggressive, profane, offensive, derogatory, or harassing are not tolerated, as well as those that are explicitly commercial advertising or those that are in some way unlawful. See the Metaculus ",
  "faq_comments_content_8": "terms of use",
  "faq_comments_content_9": "for more",
  "faq_comments_content_10": "You can ping other users using \"@username\", which will send that user a notification (if they set that option in their notification settings).",
  "faq_comments_content_11": "You are invited to upvote comments which contain relevant information to the question, and you can report comments that fail to uphold our",
  "faq_comments_content_12": "etiquette guidelines",
  "faq_comments_content_13": "If a comment is spam, inappropriate/offensive, or flagrantly breaks our rules, please send us a report (under the \"...\"menu).",
  "faq_Whatdo\"crediblesource\"and\"before[dateX]\"andsuchphrasesmeanexactly?": "What do \"credible source\" and \"before [date X]\" and such phrases mean exactly?",
  "faq_definitions_content_1": "To reduce ambiguity in an efficient way, here are some definitions that can be used in questions, with a meaning set by this FAQ:",
  "faq_definitions_content_2": "A \"credible source\" will be taken to be an online or in-print published story from a journalistic source, or information publicly posted on a the website of an organization by that organization making public information pertaining to that organization, or in another source where the preponderance of evidence suggests that the information is correct and that there is no significant controversy surrounding the information or its correctness. It will generally not include unsourced information found in blogs, facebook or twitter postings, or websites of individuals.",
  "faq_definitions_content_3": "The phrase \"Before [date X] will be taken to mean prior to the first moment at which [date X] would apply, in UTC. For example, \"Before 2010\" will be taken to mean prior to midnight January 1, 2010; \"Before June 30\" would mean prior to midnight (00:00:00) UTC June 30.",
  "faq_definitions_content_4": "Note:",
  "faq_definitions_content_5": "Previously this section used \"by [date x]\" instead of \"before [date x]\", however \"before\" is much clearer and should always be used instead of \"by\", where feasible.",
  "faq_Whattypesofquestionsarethere?": "What types of questions are there?",
  "faq_question-types_content_1": "Binary Questions",
  "faq_question-types_content_2": "Binary questions can resolve as either",
  "faq_question-types_content_3": "Yes",
  "faq_question-types_content_4": "or ",
  "faq_question-types_content_5": "No",
  "faq_question-types_content_6": "(unless the resolution criteria were underspecified or otherwise circumvented, in which case they can resolve as",
  "faq_question-types_content_7": "Ambiguous",
  "faq_question-types_content_8": "). Binary questions are appropriate when an event can either occur or not occur. For example, the question \"",
  "faq_question-types_content_9": "Will the US unemployment rate stay above 5% through November 2021?",
  "faq_question-types_content_10": "\" resolved as",
  "faq_question-types_content_11": "No",
  "faq_question-types_content_12": "because the unemployment rate dropped below 5% before the specified time.",
  "faq_question-types_content_13": "Range Questions",
  "faq_question-types_content_14": "Range questions resolve to a certain value, and forecasters can specify a probability distribution to estimate the likelihood of each value occurring. Range questions can have open or closed bounds. If the bounds are closed, probability can only be assigned to values that fall within the bounds. If one or more of the bounds are open, forecasters may assign probability outside the boundary, and the question may resolve as outside the boundary. ",
  "faq_question-types_content_15": "See here",
  "faq_question-types_content_16": "for more details about boundaries on range questions.",
  "faq_question-types_content_17": "The range interface allows you to input multiple probability distributions with different weights. ",
  "faq_question-types_content_18": "See here",
  "faq_question-types_content_19": "for more details on using the interface.",
  "faq_question-types_content_20": "There are two types of range questions, numeric range questions and date range questions.",
  "faq_question-types_content_21": "Numeric Range",
  "faq_question-types_content_22": "Numeric range questions can resolve as a numeric value. For example, the question \"",
  "faq_question-types_content_23": "What will be the 4-week average of initial jobless claims (in thousands) filed in July 2021?",
  "faq_question-types_content_24": "\" resolved as",
  "faq_question-types_content_25": ", because the underlying source reported 395 thousand initial jobless claims for July 2021.",
  "faq_question-types_content_26": "Questions can also resolve outside the numeric range. For example, the question \"",
  "faq_question-types_content_27": "What will the highest level of annualised core US CPI growth be, in 2021, according to U.S. Bureau of Labor Statistics data?",
  "faq_question-types_content_28": "\" resolved as",
  "faq_question-types_content_29": "&gt; 6.5",
  "faq_question-types_content_30": "because the underlying source reported more than 6.5% annualized core CPI growth in the US, and 6.5 was the upper bound.",
  "faq_question-types_content_31": "Date Range",
  "faq_question-types_content_32": "Date range questions can resolve as a certain date. For example, the question \"",
  "faq_question-types_content_33": "When will the next Public Health Emergency of International Concern be declared by the WHO?",
  "faq_question-types_content_34": "\" resolved as",
  "faq_question-types_content_35": "July 23, 2022",
  "faq_question-types_content_36": ", because a Public Health Emergency of International Concern was declared on that date.",
  "faq_question-types_content_37": "Questions can also resolve outside the date range. For example, the question \"",
  "faq_question-types_content_38": "When will a SpaceX Super Heavy Booster fly?",
  "faq_question-types_content_39": "\" resolved as",
  "faq_question-types_content_40": "&gt; March 29, 2022",
  "faq_question-types_content_41": "because a SpaceX Super Heavy booster was not launched before March 29, 2022, which was the upper bound.",
  "faq_Whatarequestiongroups?": "What are question groups?",
  "faq_question-groups_content_1": "Question groups are sets of closely related questions or question outcomes all collected on a single page. Forecasters can predict quickly and efficiently on these interconnected outcomes, confident that they are keeping all of their predictions internally consistent.",
  "faq_question-groups_content_2": "How do question groups facilitate more efficient, more accurate forecasting?",
  "faq_question-groups_content_3": "With question groups, it's easy to forecast progressively wider distributions the further into the future you predict to reflect increasing uncertainty. A question group collecting multiple binary questions on a limited set of outcomes or on mutually exclusive outcomes makes it easier to see which forecasts are in tension with each other.",
  "faq_question-groups_content_4": "What happens to the existing question pages when they are combined in a question group?",
  "faq_question-groups_content_5": "When regular forecast questions are converted into \"subquestions\" of a question group, the original pages are replaced by a single question group page. Comments that previously lived on the individual question pages are moved to the comment section of the newly created group page with a note indicating the move.",
  "faq_question-groups_content_6": "Do I need to forecast on every outcome / subquestion of a question group?",
  "faq_question-groups_content_7": "No. Question groups comprise multiple",
  "faq_question-groups_content_8": "independent",
  "faq_question-groups_content_9": "subquestions. For that reason, there is no requirement that you forecast on every outcome within a group.",
  "faq_question-groups_content_10": "How are question groups scored?",
  "faq_question-groups_content_11": "Each outcome or subquestion is scored in the same manner as a normal independent question.",
  "faq_question-groups_content_12": "Why don't question group outcome probabilities sum to 100%?",
  "faq_question-groups_content_13": "Even if there can only be one outcome for a particular question group, the Community Prediction functions as it would for normal independent questions. The Community Prediction will still display a median or a weighted aggregate of the forecasts on each subquestion, respectively. These medians and weighted aggregates are not constrained to sum to 100%",
  "faq_question-groups_content_14": "Feedback for question groups can be provided on the ",
  "faq_question-groups_content_15": "question group discussion post",
  "faq_WhatareConditionalPairs?": "What are Conditional Pairs?",
  "faq_conditionals_content_1": "A Conditional Pair is a special type of ",
  "faq_conditionals_content_2": "Question Group",
  "faq_conditionals_content_3": "that elicits ",
  "faq_conditionals_content_4": "conditional probabilities",
  "faq_conditionals_content_5": ". Each Conditional Pair sits between a Parent Question and a Child Question. Both Parent and Child must be existing Metaculus ",
  "faq_conditionals_content_6": "Binary Questions",
  "faq_conditionals_content_7": "Conditional Pairs ask two Conditional Questions (or \"Conditionals\" for short), each corresponding to a possible outcome of the Parent:",
  "faq_conditionals_content_8": "If the Parent resolves Yes, how will the Child resolve?",
  "faq_conditionals_content_9": "If the Parent resolves No, how will the Child resolve?",
  "faq_conditionals_content_10": "The first Conditional assumes that \"The Parent resolves Yes\" (or \"if Yes\" for short). The second conditional does the same for No.",
  "faq_conditionals_content_11": "Conditional probabilities are probabilities, so forecasting is very similar to Binary Questions. The main difference is that we present both conditionals next to each other for convenience:",
  "faq_conditionals_content_12": "Conditional questions are automatically resolved when their Parent and Child resolve:",
  "faq_conditionals_content_13": "When the Parent resolves Yes, the \"if No\" Conditional is ",
  "faq_conditionals_content_14": "Annulled",
  "faq_conditionals_content_15": ". (And vice versa.)",
  "faq_conditionals_content_16": "When the Child resolves, the Conditional that was not annulled resolves to the same value.",
  "faq_conditionals_content_17": "Let's work through an example:",
  "faq_conditionals_content_18": "The Parent is \"Will it rain today?\".",
  "faq_conditionals_content_19": "The Child is \"Will it rain tomorrow?\".",
  "faq_conditionals_content_20": "So the two Conditionals in the Conditional Pair will be:",
  "faq_conditionals_content_21": "\"If it rains today, will it rain tomorrow?\"",
  "faq_conditionals_content_22": "\"If it does not rain today, will it rain tomorrow?\"",
  "faq_conditionals_content_23": "For simplicity, Metaculus presents conditional questions graphically. In the forecasting interface they are in a table:",
  "faq_conditionals_content_24": "And in the feeds, each possible outcome of the Parent is an arrow, and each conditional probability is a bar:",
  "faq_conditionals_content_25": "Back to the example:",
  "faq_conditionals_content_26": "It rains today. The parent resolves Yes. This triggers the second conditional (\"if No\") to be annulled. It is not scored.",
  "faq_conditionals_content_27": "You wait a day. This time it doesn't rain. The Child resolves No. This triggers the remaining Conditional (\"if Yes\") to resolve No. It is scored like a normal Binary Question.",
  "faq_conditionals_content_28": "How do I create conditional pairs?",
  "faq_conditionals_content_29": "You can create and submit conditional pairs like any other question type. On the '",
  "faq_conditionals_content_30": "Create a Question",
  "faq_conditionals_content_31": "' page, select Question Type 'conditional pair' and select Parent and Child questions.",
  "faq_conditionals_content_32": "Note: You can use question group subquestions as the Parent or Child by clicking the Parent or Child button and then either searching for the subquestion in the field or pasting the URL for the subquestion.",
  "faq_conditionals_content_33": "To copy the URL for a subquestion, simply visit a question group page and click the '...' more options menu to reveal the Copy Link option.",
  "faq_HowdoIfindcertainquestionsonMetaculus?": "How do I find certain questions on Metaculus?",
  "faq_navigation-and-filtering_content_1": "Questions on Metaculus are sorted by activity by default. Newer questions, questions with new comments, recently upvoted questions, and questions with many new predictions will appear at the top of the ",
  "faq_navigation-and-filtering_content_2": "Metaculus homepage",
  "faq_navigation-and-filtering_content_3": ". However, there are several additional ways to find questions of interest and customize the way you interact with Metaculus.",
  "faq_SearchBar": "Search Bar",
  "faq_search-bar_content_1": "The search bar can be used to find questions using keywords and semantic matches. At this time it cannot search comments or users.",
  "faq_Filters": "Filters",
  "faq_filters_content_1": "Questions can be sorted and filtered in a different manner from the default using the filters menu. Questions can be filtered by type, status and participation. Questions can also be ordered, for example by \"Newest\". Note that the options available change when different filters are selected. For example, if you filter by \"Closed\" questions you will then be shown an option to order by \"Soonest Resolving\".",
  "faq_QuestionResolution": "Question Resolution",
  "faq_Whatarethe\"opendate\",\"closedate\"and\"resolvedate?\"": "What are the \"open date\", \"close date\" and \"resolve date?\"",
  "faq_closers_content_1": "When submitting a question, you are asked to specify the closing date (when the question is no longer available for predicting) and resolution date (when the resolution is expected to occur). The date the question is set live for others to forecast on is known as the open date.",
  "faq_closers_content_2": "The",
  "faq_closers_content_3": "open date",
  "faq_closers_content_4": "is the date/time when the question is open for predictions. Prior to this time, if the question is active, it will have \"upcoming\" status, and is potentially subject to change based on feedback. After the open date, changing questions is highly discouraged (as it could change details which are relevant to forecasts that have already been submitted) and such changes are typically noted in the question body and in the comments on the question",
  "faq_closers_content_5": "The",
  "faq_closers_content_6": "close date",
  "faq_closers_content_7": "is the date/time after which predictions can no longer be updated.",
  "faq_closers_content_8": "The",
  "faq_closers_content_9": "resolution date",
  "faq_closers_content_10": "is the date when the event being predicted is expected to have definitively occurred (or not). This date lets Metaculus Admins know when the question might be ready for resolution. However, this is often just a guess, and is not binding in any way.",
  "faq_closers_content_11": "In some cases, questions must resolve at the resolution date according to the best available information. In such cases, it becomes important to choose the resolution date carefully. Try to set resolution dates that make for interesting and insightful questions! The date or time period the question is asking about must always be explicitly mentioned in the text (for example, \"this question resolves as the value of X on January 1, 2040, according to source Y\" or \"this question resolves as",
  "faq_closers_content_12": "Yes",
  "faq_closers_content_13": "if X happens before January 1, 2040)\".",
  "faq_closers_content_14": "The close date",
  "faq_closers_content_15": "must",
  "faq_closers_content_16": "be at least one hour prior to the resolution date, but can be much earlier, depending upon the context. Here are some guidelines for specifying the close date:",
  "faq_closers_content_17": "If the outcome of the question will very likely or assuredly be determined at a fixed known time, then the closing time should be immediately before this time, and the resolution time just after that. (Example: a scheduled contest between competitors or the release of scheduled data)",
  "faq_closers_content_18": "If the outcome of a question will be determined by some process that will occur at an unknown time, but the outcome is likely to be independent of this time, then it should be specified that the question ",
  "faq_closers_content_19": "retroactively closes",
  "faq_closers_content_20": "some appropriate time before the process begins. (Example: success of a rocket launch occurring at an unknown time)",
  "faq_closers_content_21": "If the outcome of a question depends on a discrete event that may or may not happen, the close time should be specified as shortly before the resolve time. The resolve time is chosen based on author discretion of the period of interest.",
  "faq_closers_content_22": "Note:",
  "faq_closers_content_23": "Previous guidance suggested that a question should close between 1/2 to 2/3 of the way between the open time and resolution time. This was necessary due to the scoring system at the time, but has been replaced by the above guidelines due to an ",
  "faq_closers_content_24": "update to the scoring system",
  "faq_Whattimezoneisusedforquestions?": "What timezone is used for questions?",
  "faq_timezone_content_1": "For dates and times written in the question, such as \"will event X happen before January 1, 2030?\", if the timezone is not specified ",
  "faq_timezone_content_2": "Coordinated Universal Time (UTC)",
  "faq_timezone_content_3": " will be used. Question authors are free to specify a different timezone in the resolution criteria, and any timezone specified in the text will be used.",
  "faq_timezone_content_4": "For",
  "faq_timezone_content_5": "date range",
  "faq_timezone_content_6": "questions, the dates on the interface are in UTC. Typically the time of day makes little difference as one day is miniscule in comparison to the full range, but occasionally for shorter term questions the time of day might materially impact scores. If it is not clear what point in a specified period a date range question will be resolved as, it resolves as the ",
  "faq_timezone_content_7": "midpoint of that period",
  "faq_timezone_content_8": ". For example, if a question says it will resolve as a certain day, but not what time of day, it will resolve as noon UTC on that day.",
  "faq_Whodecidestheresolutiontoaquestion?": "Who decides the resolution to a question?",
  "faq_who-resolves_content_1": "Only Metaculus Administrators can resolve questions. Binary questions can resolve",
  "faq_who-resolves_content_2": "Yes",
  "faq_who-resolves_content_3": "No",
  "faq_who-resolves_content_4": "Ambiguous, or Annuled",
  "faq_who-resolves_content_5": ". Range questions can resolve to a specific value, an out-of-bounds value, ",
  "faq_who-resolves_content_6": "Ambiguous, or Annuled",
  "faq_Whatare\"Ambiguous\"and\"Annulled\"resolutions?": "What are \"Ambiguous\" and \"Annulled\" resolutions?",
  "faq_ambiguous-annulled_content_1": "Sometimes a question cannot be resolved because the state of the world, the",
  "faq_ambiguous-annulled_content_2": "truth of the matter",
  "faq_ambiguous-annulled_content_3": ", is too uncertain. In these cases, the question is resolved as Ambiguous.",
  "faq_ambiguous-annulled_content_4": "Other times, the state of the world is clear, but a key assumption of the question was overturned. In these cases, the question is Annulled.",
  "faq_ambiguous-annulled_content_5": "In the same way, when a Conditional turns out to be based on an outcome that did not occur, it is Annulled. For example, when a ",
  "faq_ambiguous-annulled_content_6": "Conditional Pair",
  "faq_ambiguous-annulled_content_7": "'s parent resolves Yes, the",
  "faq_ambiguous-annulled_content_8": "if No",
  "faq_ambiguous-annulled_content_9": "Conditional is Annulled.",
  "faq_ambiguous-annulled_content_10": "When questions are Annulled or resolved as Ambiguous, they are no longer open for forecasting, and they are not scored.",
  "faq_ambiguous-annulled_content_11": "If you'd like to read more about why Ambiguous and Annulled resolutions are necessary you can expand the section below.",
  "faq_ambiguous-annulled_content_12": "Reasons for Ambiguous and Annulled resolutions",
  "faq_WhywasthisquestionAnnulledorresolvedasAmbiguous?": "Why was this question Annulled or resolved as Ambiguous?",
  "faq_reason-annulled_content_1": "An Ambiguous or Annulled resolution generally implies that there was some inherent ambiguity in the question, that real-world events subverted one of the assumptions of the question, or that there is not a clear consensus as to what in fact occurred. Metaculus strives for satisfying resolutions to all questions, and we know that Ambiguous and Annulled resolutions are disappointing and unsatisfying. However, when resolving questions we have to consider factors such as fairness to all participating forecasters and the underlying incentives toward accurate forecasting.",
  "faq_reason-annulled_content_2": "To avoid this unfairness and provide the most accurate information, we resolve all questions in accordance with the actual written text of the resolution criteria whenever possible. By adhering as closely as possible to a reasonable interpretation of what's written in the resolution criteria, we minimize the potential for forecasters to arrive at different interpretations of what the question is asking, which leads to fairer scoring and better forecasts. In cases where the outcome of a question does not clearly correspond to the direction or assumptions of the text of the resolution criteria, Ambiguous resolution or Annulling the question allows us to preserve fairness in scoring.",
  "faq_TypesofAmbiguousorAnnulledResolutions": "Types of Ambiguous or Annulled Resolutions",
  "faq_types-annulled_content_1": "A question's resolution criteria can be thought of as akin to a legal contract. The resolution criteria create a shared understanding of what forecasters are aiming to predict, and define the method by which they agree to be scored for accuracy when choosing to participate. When two forecasters who have diligently read the resolution criteria of a question come away with significantly different perceptions about the meaning of that question, it creates unfairness for at least one of these forecasters. If both perceptions are reasonable interpretations of the text, then one of these forecasters will likely receive a poor score at resolution time through no fault of their own. Additionally, the information provided by the forecasts on the question will be poor due to the differing interpretations.",
  "faq_types-annulled_content_2": "The following sections provide more detail about common reasons we resolve questions as Ambiguous or Annul them and some examples. Some of these examples could fit into multiple categories, but we've listed them each in one main category as illustrative examples. This list of types of Ambiguous or Annulled resolutions is not exhaustive &mdash; there are other reasons that a question may resolve Ambiguous or be Annulled &mdash; but these cover some of the more common and some of the trickier scenarios. Here's a condensed version, but read on for more details:",
  "faq_types-annulled_content_3": "Ambiguous resolution",
  "faq_types-annulled_content_4": "Reserved for questions where reality is not clear.",
  "faq_types-annulled_content_5": "No clear consensus",
  "faq_types-annulled_content_6": "There is not enough information available to arrive at an appropriate resolution.",
  "faq_types-annulled_content_7": "Annulment",
  "faq_types-annulled_content_8": "Reserved for questions where the reality is clear but the question is not.",
  "faq_types-annulled_content_9": "Underspecified questions",
  "faq_types-annulled_content_10": "The question did not clearly describe an appropriate method to resolve the question.",
  "faq_types-annulled_content_11": "Subverted assumptions",
  "faq_types-annulled_content_12": "The question made assumptions about the present or future state of the world that were violated.",
  "faq_types-annulled_content_13": "Imbalanced outcomes and consistent incentives",
  "faq_types-annulled_content_14": "The binary question did not adequately specify a means for either Yes or No resolution, leading to imbalanced outcomes and bad incentives.",
  "faq_types-annulled_content_15": "Note:",
  "faq_types-annulled_content_16": "Previously Metaculus only had one resolution type &mdash; Ambiguous &mdash; for cases where a question could not otherwise be resolved. We've since separated these into two types &mdash; Ambiguous and Annulled &mdash; to provide clarity on the reason that a question could not otherwise be resolved. Annulling questions first became an option in April of 2023.",
  "faq_AmbiguousResolution": "Ambiguous Resolution",
  "faq_ambiguous-details_content_1": "Ambiguous resolution is reserved for questions where reality is not clear. Either because reporting about an event is conflicted or unclear about what actually happened, or available material is silent on the information being sought. We've described the types of questions where Ambiguous resolution is appropriate as those with",
  "faq_ambiguous-details_content_2": "No Clear Consensus",
  "faq_NoClearConsensus": "No Clear Consensus",
  "faq_no-clear-consensus_content_1": "Questions can also resolve Ambiguous when there is not enough information available to arrive at an appropriate resolution. This can be because of conflicting or unclear media reports, or because a data source that was expected to provide resolution information is no longer available. The following are some examples where there was no clear consensus.",
  "faq_no-clear-consensus_content_2": "Will Russian troops enter Kyiv, Ukraine before December 31, 2022?",
  "faq_no-clear-consensus_content_3": "This question asked if at least 100 Russian troops would enter Ukraine before the end of 2022. It was clear that some Russian troops entered Ukraine, and even probable that there were more than 100 Russian troops in Ukraine. However there was no clear evidence that could be used to resolve the question, so it was necessary to resolve as Ambiguous. In addition to the lack of a clear consensus, this question is also an example of imbalanced outcomes and the need to preserve incentives. ",
  "faq_no-clear-consensus_content_4": "As an Admin explains here",
  "faq_no-clear-consensus_content_5": ", due to the uncertainty around events in February the question could not remain open to see if a qualifying event would happen before the end of 2022. This is because the ambiguity around the events in February would necessitate that the question could only resolve as Yes or Ambiguous, which creates an incentive to forecast confidently in an outcome of Yes.",
  "faq_no-clear-consensus_content_6": "What will the average cost of a ransomware kit be in 2022?",
  "faq_no-clear-consensus_content_7": "This question relied on data published in a report by Microsoft, however Microsoft's report for the year in question no longer contained the relevant data. It's ",
  "faq_no-clear-consensus_content_8": "Metaculus policy",
  "faq_no-clear-consensus_content_9": "that by default if a resolution source is not available Metaculus may use a functionally equivalent source in its place unless otherwise specified in the resolution text, but for this question a search for alternate sources did not turn anything up, leading to Ambiguous resolution.",
  "faq_Annulment": "Annulment",
  "faq_annulment-details_content_1": "Annulling a question is reserved for situations where reality is clear but the question is not. In other words, the question failed to adequately capture a method for clear resolution.",
  "faq_annulment-details_content_2": "Note:",
  "faq_annulment-details_content_3": "Annulment was introduced in April of 2023, so while the following examples describe Annulment the questions in actuality were resolved as Ambiguous.",
  "faq_TheQuestionWasUnderspecified": "The Question Was Underspecified",
  "faq_annulled-underspecified_content_1": "Writing good forecasting questions is hard, and it only gets harder the farther the question looks into the future. To fully eliminate the potential for a question to be Annulled the resolution criteria must anticipate all the possible outcomes that could occur in the future; in other words, there must be clear direction for how the question resolves in every possible outcome. Most questions, even very well-crafted ones, can't consider",
  "faq_annulled-underspecified_content_2": "every",
  "faq_annulled-underspecified_content_3": "possible outcome. When an outcome occurs that does not correspond to the instructions provided in the resolution criteria of the question then that question may have to be Annulled. In some cases we may be able to find an interpretation that is clearly an appropriate fit for the resolution criteria, but this is not always possible.",
  "faq_annulled-underspecified_content_4": "Here are some examples of Annulment due to underspecified questions:",
  "faq_annulled-underspecified_content_5": "What will Substack's Google Trends index be at end of 2022?",
  "faq_annulled-underspecified_content_6": "This question did not clearly specify how Google trends would be used to arrive at the average index for December of 2022, because the index value depends on the date range specified in Google Trends. An Admin provided more details in ",
  "faq_annulled-underspecified_content_7": "this comment",
  "faq_annulled-underspecified_content_8": "When will a fusion reactor reach ignition?",
  "faq_annulled-underspecified_content_9": "This question did not clearly define what was meant by &ldquo;ignition&rdquo;. As an Admin described in ",
  "faq_annulled-underspecified_content_10": "this comment",
  "faq_annulled-underspecified_content_11": ", the definition of ignition may vary depending on the researchers using it and the fusion method, as well as the reference frame for what counts as an energy input and output.",
  "faq_annulled-underspecified_content_12": "Will Russia order a general mobilization by January 1, 2023?",
  "faq_annulled-underspecified_content_13": "This question asked about Russia ordering a general mobilization, but the difficult task of determining that a general mobilization was ordered was not adequately addressed in the resolution criteria. The text of the question asked about a &ldquo;general mobilization&rdquo;, but the definitions used in the resolution criteria differed from the common understanding of a &ldquo;general mobilization&rdquo; and didn&rsquo;t adequately account for the actual partial mobilization that was eventually ordered, as ",
  "faq_annulled-underspecified_content_14": "explained by an Admin here",
  "faq_TheAssumptionsoftheQuestionWereSubverted": "The Assumptions of the Question Were Subverted",
  "faq_annulled-subverted_content_1": "Questions often contain assumptions in their resolution criteria, many of which are unstated. For example, assuming that the underlying methodology of a data source will remain the same, assuming that an organization will provide information about an event, or assuming that an event would play out a certain way. The best practice is to specify what happens in the event certain assumptions are violated (including by specifying that the question will be Annulled in certain situations) but due to the difficulty in anticipating these outcomes this isn't always done.",
  "faq_annulled-subverted_content_2": "Here are some examples of Annulment due to subverted assumptions:",
  "faq_annulled-subverted_content_3": "Will a technical problem be identified as the cause of the crash of China Eastern Airlines Flight 5735?",
  "faq_annulled-subverted_content_4": "This question relied on the conclusions of a future National Transportation Safety Board (NTSB) report. However, it was a Chinese incident so it was unlikely that the NTSB would publish such a report. Additionally, the question did not specify a date by which the report must be published resulting in a resolution of No. Since this was not specified and the assumption of a future NTSB report was violated the question was Annulled, as ",
  "faq_annulled-subverted_content_5": "explained by an Admin here",
  "faq_annulled-subverted_content_6": "What will the Federal Reserves' Industrial Production Index be for November 2021, for semiconductors, printed circuit boards and related products?",
  "faq_annulled-subverted_content_7": "This question did not provide a description of how it should resolve in the event the underlying source changed its methodology. It anticipated the possibility of the base period changing, however, the entire methodology used to construct the series changed before this question resolved, not just the base period. Because the unwritten assumption of a consistent methodology was violated, the question was Annulled.",
  "faq_annulled-subverted_content_8": "When will Russia's nuclear readiness scale return to Level 1?",
  "faq_annulled-subverted_content_9": "Media reporting about Russia's nuclear readiness level gave the impression that the level had been changed to level 2, leading to the creation of this question. However, a more thorough investigation found that Russia's nuclear readiness most likely did not change. This violated the assumption of the question leading to the question being Annulled, as ",
  "faq_annulled-subverted_content_10": "explained by an Admin here",
  "faq_annulled-subverted_content_11": "What will be the Biden Administration's social cost of 1 ton of CO2 in 2022?",
  "faq_annulled-subverted_content_12": "This question specified that it would resolve according to a report published by the US Interagency Working Group (IWG), however the IWG did not publish an estimate before the end of 2022. This question anticipated this outcome and appropriately specified that it should be Annulled if no report was published before the end of 2022, and the question was resolved accordingly.",
  "faq_ImbalancedOutcomesandConsistentIncentives": "Imbalanced Outcomes and Consistent Incentives",
  "faq_annulled-imbalanced_content_1": "Sometimes questions imply imbalanced outcomes, for example where the burden of proof for an event to be considered to have occurred is high and tips the scales toward a binary question resolving No, or where the question would require a substantial amount of research to surface information showing that an event occurred, which also favors a resolution of No. In certain circumstances these kinds of questions are okay, so long as there is a clear mechanism for the question to resolve as Yes and to resolve as No. However, sometimes questions are formulated such that there's no clear mechanism for a question to resolve as No, leading to the only realistic outcomes being a resolution of Yes or Annulled. This creates a bias in the question and also produces bad incentives if the question isn't Annulled.",
  "faq_annulled-imbalanced_content_2": "The case of imbalanced outcomes and consistent incentives is best explained with examples, such as the following:",
  "faq_annulled-imbalanced_content_3": "Will any prediction market cause users to lose at least $1M before 2023?",
  "faq_annulled-imbalanced_content_4": "This question asks whether certain incidents such as hacking, scams, or incorrect resolution lead to users losing $1 million or more from a prediction market. However, there's no clear mechanism specified to find information about this, as prediction markets aren't commonly the subject of media reports. Concretely proving that this did not occur would require extensive research. This creates an imbalance in the resolution criteria. The question would resolve as Yes if there was a clear report from credible sources that this occurred. However, to resolve as No it would require extensive research to confirm that it didn't occur and a knowledge of the happenings in prediction markets that most people do not possess. To resolve as No Metaculus would either have to do an absurd amount of research, or assume that a lack of prominent reports on the topic is sufficient to resolve as No. In this case the question had to be Annulled.",
  "faq_annulled-imbalanced_content_5": "Now consider if there had been a clear report that this had actually occurred. In a world where that happened the question could arguably have been resolved as Yes. However, savvy users who follow our methods on Metaculus might realize that when a mechanism for a No resolution is unclear, that the question will then resolve as Yes or be Annulled. This creates bad incentives, as these savvy forecasters might begin to raise the likelihood of Yes resolution on future similar forecasts as they meta-predict how Metaculus handles these questions. For this reason, binary questions must have a clear mechanism for how they resolve as both Yes and No. If the mechanism is unclear, then it can create bad incentives. Any questions without a clear mechanism to resolve as both possible outcomes should be Annulled, even if a qualifying event occurs that would resolve the question as Yes.",
  "faq_annulled-imbalanced_content_6": "Will any remaining FTX depositor withdraw any amount of tradeable assets from FTX before 2023?",
  "faq_annulled-imbalanced_content_7": "This question asked if an FTX depositor would withdraw assets where the withdrawal was settled by FTX. Unfortunately this question required a knowledge of the details of FTX withdrawals that was unavailable to Admins, resulting in there being no real mechanism to resolve the question as No. This led to an imbalance in possible outcomes, where the question could only truly resolve as Yes or be Annulled. The imbalance necessitated that the question be resolved as Ambiguous to preserve consistent incentives for forecasting.",
  "faq_Doallquestionsgetresolved?": "Do all questions get resolved?",
  "faq_allres_content_1": "Currently, all questions will be resolved.",
  "faq_Whenwillaquestionberesolved?": "When will a question be resolved?",
  "faq_whenresolve_content_1": "Questions will be resolved when they have satisfied the criteria specified in the resolution section of the question (or conversely, when those criteria have conclusively failed to be met). Each question also has a \"Resolution Date\" listed in our system for purposes such as question sorting; however, this listed date is often nothing more than an approximation, and the actual date of resolution may not be known in advance.",
  "faq_whenresolve_content_2": "For questions which ask when something will happen (such as ",
  "faq_whenresolve_content_3": "When will the first humans land successfully on Mars?",
  "faq_whenresolve_content_4": "), forecasters are asked to predict the date/time when the criteria have been satisfied (though the question may be decided and points awarded at some later time, when the evidence is conclusive). Some questions predict general time intervals, such as \"In which month will unemployment pass below 4%?\"; when such a question has specified the date/time which will be used, those terms will be used. If these terms are not given, the default policy will be to resolve as the",
  "faq_whenresolve_content_5": "midpoint of that period",
  "faq_whenresolve_content_6": "(for example, if the January report is the first month of unemployment under 4%, the resolution date will default to January 15).",
  "faq_Isthebackgroundmaterialusedforquestionresolution?": "Is the background material used for question resolution?",
  "faq_resolvebackground_content_1": "No, only the resolution criteria is relevant for resolving a question, the background section is intended only to provide potentially useful information and context for forecasters. In a well-specified question, the resolution criteria should stand on its own as a set of self-contained instructions for resolving the question. In rare cases or older questions on Metaculus, the background material may be necessary to inform resolution, but the information in the resolution criteria supersedes conflicting information in the background material.",
  "faq_resolvebackground_content_2": "Still, we want the background material to be as helpful as possible and accurately capture the context and relevant information available at the time the question was written, so if you see errors or misleading information in the background of a question please let Admins know by tagging @admins in a comment!",
  "faq_Whathappensiftheresolutioncriteriaofaquestionisunclearorsuboptimal?": "What happens if the resolution criteria of a question is unclear or suboptimal?",
  "faq_unclearresolve_content_1": "We take care to launch questions that are as clearly specified as possible. Still, writing clear and objectively resolvable questions is challenging, and in some cases a question's resolution criteria may unintentionally allow for multiple different interpretations or may not accurately represent the question being asked. In deciding how to approach questions that have been launched with these deficiencies, Admins primarily consider fairness to forecasters. Issuing clarifications or edits for open questions can harm some forecaster's scores when the clarification significantly changes the meaning of the question. Based on an assessment of fairness and other factors, Admins may issue a clarification to an open question to better specify the meaning. This is typically most appropriate when a question has not been open for long (and therefore forecasts can be updated with negligible impact to scores), when a question contains inconsistent or conflicting criteria, or when the clarification adds specificity where there previously was none in a way that avoids substantial changes to the meaning.",
  "faq_unclearresolve_content_2": "In many cases such questions must be resolved as ",
  "faq_unclearresolve_content_3": "ambiguous or annulled",
  "faq_unclearresolve_content_4": "to preserve fairness in scoring. If you believe there are ambiguities or conflicts in the resolution criteria for a question please let Admins know by tagging @admins in a comment. We hope inconsistencies can be identified as early as possible in the lifetime of a question so that they can be addressed. Claims of unclear resolution criteria made for questions which have already closed or claims of incorrect resolution for questions which have already resolved will be held to a higher standard of evidence if the issue(s) with the resolution criteria was not previously mentioned while the question was open to forecasting.",
  "faq_Canquestionsbere-resolved?": "Can questions be re-resolved?",
  "faq_reresolve_content_1": "Yes, at times questions are resolved and it is later discovered these resolutions were in error given the information available at the time. Some questions may even specify that they will be resolved according to initial reporting or results, but specify re-resolution in the event that the final results disagree with the initial results (for example, questions about elections might use this mechanism to allow prompt feedback for forecasters but arrive at the correct answer in the rare event that the initial election call was wrong). Questions may be re-resolved in such cases if Metaculus determines that re-resolution is appropriate.",
  "faq_Whathappensifaquestiongetsresolvedintherealworldpriortotheclosetime?": "What happens if a question gets resolved in the real world prior to the close time?",
  "faq_whatifres_content_1": "When resolving a question, the Moderator has an option to change the effective closing time of a question, so that if the question is unambiguously resolved prior to the closing time, the closing time can be changed to a time prior to which the resolution is uncertain.",
  "faq_whatifres_content_2": "When a question closes early, the points awarded are",
  "faq_whatifres_content_3": "only",
  "faq_whatifres_content_4": " those accumulated up until the (new) closing time. This is necessary in order to keep scoring \"proper\" (i.e. maximally reward predicting the right probability) and prevent gaming of points, but it does mean that the overall points (positive or negative) may end up being less than expected.",
  "faq_Whenshouldaquestionspecifyretroactiveclosure?": "When should a question specify retroactive closure?",
  "faq_retroactive-closure_content_1": "In some cases when the timing of an event is unknown it may be appropriate to change the closing date to a time before the question resolved, after the resolution is known. This is known as retroactive closure. Retroactive closure is not allowed except in the case of an event where the timing of the event is unknown and the outcome of the event is independent of the timing of the event, as described in the question closing guidelines above. When the timing of the event impacts the outcome of the event retroactive closure would violate proper scoring. For scoring to be proper a question must only close retroactively when the outcome is independent of the timing of the event. Here are several examples:",
  "faq_retroactive-closure_content_2": "The date of a rocket launch can often vary based on launch windows and weather, and the success or failure of the launch is primarily independent of when the launch occurs. ",
  "faq_retroactive-closure_content_3": "In this case retroactive closure is appropriate",
  "faq_retroactive-closure_content_4": ", as the timing of the launch is very unlikely to impact forecasts for the success of the launch.",
  "faq_retroactive-closure_content_5": "In some countries elections can be called earlier than scheduled (these are known as ",
  "faq_retroactive-closure_content_6": "snap elections",
  "faq_retroactive-closure_content_7": "). The timing of snap elections is often up to the party in power, and elections are often scheduled at a time the incumbent party considers to be favorable to their prospects. ",
  "faq_retroactive-closure_content_8": "In this case retroactive closure is",
  "faq_retroactive-closure_content_9": "not",
  "faq_retroactive-closure_content_10": "appropriate",
  "faq_retroactive-closure_content_11": ", as the timing of the election will impact forecasts for the outcome of the election, violating proper scoring.",
  "faq_retroactive-closure_content_12": "Previously some questions on Metaculus were approved with inappropriate retroactive closure clauses. For example, the question \"",
  "faq_retroactive-closure_content_13": "When will the number of functional artificial satellites in orbit exceed 5,000?",
  "faq_retroactive-closure_content_14": "\" specifies retroactive closure to the date when the 5,001st satellite is launched. ",
  "faq_retroactive-closure_content_15": "In this case retroactive closure was",
  "faq_retroactive-closure_content_16": "not",
  "faq_retroactive-closure_content_17": "appropriate",
  "faq_retroactive-closure_content_18": ", because the resolution of the question was dependent on the closure date since both relied on the number of satellites launched.",
  "faq_retroactive-closure_content_19": "Forecasters often like retroactive closure because it prevents points from being truncated when an event occurs before the originally scheduled close date. But in order to elicit the best forecasts it’s important to follow proper scoring rules. For more on point truncation ",
  "faq_retroactive-closure_content_20": "this section of the FAQ",
  "faq_retroactive-closure_content_21": "While Metaculus will try not to approve questions which specify inappropriate retroactive closure, sometimes new or existing questions do specify it. It is the policy of Metaculus to ignore inappropriate retroactive closure when resolving questions.",
  "faq_Whathappensifaquestionsresolutioncriteriaturnouttohavebeenfulfilledpriortotheopeningtime?": "What happens if a question's resolution criteria turn out to have been fulfilled prior to the opening time?",
  "faq_whatifres2_content_1": "Our Moderators and question authors strive to be as clear and informed as possible on each question, but mistakes occasionally happen, and will be decided by our Admins' best judgement. For a hypothetical question like ",
  "faq_whatifres2_content_2": "Will a nuclear detonation occur in a Japanese City by 2030?",
  "faq_whatifres2_content_3": " it can be understood by common sense that we are asking about the ",
  "faq_whatifres2_content_4": "next",
  "faq_whatifres2_content_5": "detonation after the detonations in 1945. In other questions like ",
  "faq_whatifres2_content_6": "Will Facebook implement a feature to explain news feed recommendations before 2026?",
  "faq_whatifres2_content_7": ", we are asking about the",
  "faq_whatifres2_content_8": "first",
  "faq_whatifres2_content_9": "occurrence of this event. Since this event occurred before the question opened and this was not known to the question author, the question resolved ambiguously.",
  "faq_Whathappensifaresolutionsourceisnolongeravailable?": "What happens if a resolution source is no longer available?",
  "faq_ressrc_content_1": "There are times when the intent of a question is to specifically track the actions or statements of specific organizations or people (such as, \"how many Electoral Votes will the Democrat win in the 2020 US Presidential Election ",
  "faq_ressrc_content_2": "according to the Electoral College",
  "faq_ressrc_content_3": "\"); at other times, we are interested only in the actual truth, and we accept a resolution source as being an acceptable approximation (such as, \"how many COVID-19 deaths will there be in the US in 2021 according to the CDC?\"). That said, in many cases it is not clear which is intended.",
  "faq_ressrc_content_4": "Ideally, every question would be written with maximally clear language, but some ambiguities are inevitable. Unless specifically indicated otherwise, if a resolution source is judged by Metaculus Admins to be defunct, obsolete, or inadequate, Admins will make a best effort to replace it with a functional equivalent. Questions can over-rule this policy with language such as \"If [this source] is no longer available, the question will resolve Ambiguously\" or \"This question tracks publications by [this source], regardless of publications by other sources.\"",
  "faq_WhatareResolutionCouncils?": "What are Resolution Councils?",
  "faq_rescouncil_content_1": "Metaculus uses Resolution Councils to reduce the likelihood of ambiguous resolutions for important questions—those that we feel have the potential to be in the top 1% of all questions on the platform in terms of impact.",
  "faq_rescouncil_content_2": "A Resolution Council is an individual or group that is assigned to resolve a question. Resolution Council questions resolve at the authority of the individual or individuals identified in the resolution criteria. These individuals will identify the resolution that best aligns with the question and its resolution criteria.",
  "faq_rescouncil_content_3": "If a Resolution Council member is not available to resolve a question, Metaculus may choose a suitable replacement.",
  "faq_Predictions": "Predictions",
  "faq_Isthereatutorialorwalkthrough?": "Is there a tutorial or walkthrough?",
  "faq_tutorial_content_1": "We are working on recreating the tutorial. {/* Yes! Start the Metaculus forecasting tutorial ",
  "faq_tutorial_content_2": "here",
  "faq_HowdoImakeaprediction?CanIchangeitlater?": "How do I make a prediction? Can I change it later?",
  "faq_howpredict_content_1": "You make a prediction simply by sliding the slider on the question's page to the probability you believe most captures the likelihood that the event will occur.",
  "faq_howpredict_content_2": "You can revise your prediction at any time up until the question closes, and you are encouraged to do so: as new information comes to light, it is beneficial to take it into account.",
  "faq_HowcanIwithdrawmyprediction?": "How can I withdraw my prediction?",
  "faq_howwithdraw_content_1": "If you've made a prediction on a question and it is still open for predicting, you can withdraw your prediction by pressing the \"withdraw\" button. For group questions, the \"withdraw\" button is nested in the \"...\" menu next to the option you wish to withdraw.",
  "faq_howwithdraw_content_2": "Once you’ve withdrawn, you no longer have a prediction standing for that question. Of course, after withdrawing, you can make a new prediction any time and start accruing scores again. Concretely, this means that, starting from the moment you withdrew and until you make a new prediction:",
  "faq_howwithdraw_content_3": "You stop accruing scores, including Peer scores, Baseline scores, etc.",
  "faq_howwithdraw_content_4": "You stop accruing Coverage for the Peer leaderboards.",
  "faq_howwithdraw_content_5": "You aren’t part of the Community Prediction or other aggregates.",
  "faq_howwithdraw_content_6": "None of those behaviours are retroactive: you still get scores and coverage for times up until you withdrew, and your past predictions are not removed from the Community Prediction.",
  "faq_howwithdraw_content_7": "Let’s work through an example. A 5-day question has 3 forecasters, Alex, Bailey and Cedar, who makes these forecasts:",
  "faq_howwithdraw_content_17": "As you can see, Bailey withdraws at the end of the third day, and Cedar only joins the second day. This changes the community prediction on day 4: it is now 50%. But this does not retroactively change the Community prediction on day 2: it is still the case that it was 60% on day 2.",
  "faq_howwithdraw_content_18": "To complete the example, let’s say the question resolves Yes. Here are the scores and coverages that each forecaster will get for each day:",
  "faq_howwithdraw_content_33": "Bailey did not get any scores for days 4 and 5 when they were withdrawn, just like Cedar did not get any scores for day 1 before they made their first prediction.",
  "faq_howwithdraw_content_34": "To quickly see which questions you’ve ever predicted, withdrawn from, or still have a standing prediction on, there are filters in the filter menu:",
  "faq_howwithdraw_content_35": "And withdrawals show up as crosses in the timeline graphs:",
  "faq_HowdoIusetherangeinterface?": "How do I use the range interface?",
  "faq_range-interface_content_1": "Some Metaculus questions allow numeric or date range inputs, where you specify the distribution of probability you think is likely over a possible range of outcomes. This probability distribution is known as a ",
  "faq_range-interface_content_2": "probability density function",
  "faq_range-interface_content_3": " and is the probability per unit of length. The probability density function can be used to determine the probability of a value falling within a range of values.",
  "faq_range-interface_content_4": "When you hover over the chart you see the probabilities at each point at the bottom of the chart. For example, in the image below you can see the probability density at the value 136, denoted by \"P(x = 136)\", and you can see the probability density that you and the community have assigned to that point (in the image the user has assigned a probability density of 1.40 to that value and the community has assigned a probability density of 2.97).",
  "faq_range-interface_content_5": "By selecting the \"Probability Density\" dropdown at the top of the chart you can change the display to \"Cumulative Probability\". This display shows the ",
  "faq_range-interface_content_6": "cumulative distribution function",
  "faq_range-interface_content_7": ", or in other words for any point it shows you the probability that you and the community have assigned to the question resolving below the indicated value. For example, in the image below you can see the probability that you and the community have assigned to the question resolving below the value of 136, denoted by \"P(x &lt; 136)\". The probability that the user has assigned is 7% to the question resolving below that value, while the community has assigned an 83% chance to the question resolving below that value.",
  "faq_range-interface_content_8": "The vertical lines shown on the graphs indicate the 25th percentile, median, and 75th percentile forecasts, respectively, of the user and the community. These values are also shown for the user and the community in the table at the bottom.",
  "faq_OutofBoundsResolution": "Out of Bounds Resolution",
  "faq_out-of-bounds-resolution_content_1": "In the table showing the predictions at the bottom of the images above, you will see that in addition to the 25th percentile, median, and 75th percentile probabilities there is also one labeled \"&gt; 500\". This question has an open upper bound, which means forecasters can assign a probability that the question will resolve as a value above the upper end of the specified range. For the question depicted above the community and the forecaster each assign a 1% probability to the question resolving above the upper boundary.",
  "faq_out-of-bounds-resolution_content_2": "Questions can have open or closed boundaries on either end of the specified range.",
  "faq_ClosedBoundaries": "Closed Boundaries",
  "faq_closed-boundaries_content_1": "A closed boundary means forecasters are restricted from assigning a probability beyond the specified range. Closed boundaries are appropriate when a question cannot resolve outside the range. For example, a question asking what share of the vote a candidate will get with a range from 0 to 100 should have closed boundaries because it is not possible for the question to resolve outside the range. Closed boundaries restrict forecasters from assigning probabilities outside the specified range.",
  "faq_OpenBoundaries": "Open Boundaries",
  "faq_open-boundaries_content_1": "An open boundary allows a question to resolve outside the range. For example, a question asking what share of the vote a candidate will get with a range from 30 to 70 should have open boundaries because it is possible that the candidate could get less than 30% of the vote or more than 70%. Open boundaries should be specified even if it unlikely that the vote share falls outside the range, because it is theoretically possible that vote shares outside the specified range can occur.",
  "faq_open-boundaries_content_2": "Forecasters can assign probabilities outside the range when the boundary is open by moving the slider all the way to one side. The weight can also be lowered or increased to adjust the probability assigned to an out of bounds resolution.",
  "faq_MultipleComponents": "Multiple Components",
  "faq_multiple-components_content_1": "In the images shown above you can see that the user has assigned two probability distributions. Up to five logistic distributions can be added using the \"Add Component\" button. The relative weight of each can be adjusted using the \"weight\" slider below each component.",
  "faq_HowistheCommunityPredictioncalculated?": "How is the Community Prediction calculated?",
  "faq_community-prediction_content_1": "The Community Prediction is a consensus of recent forecaster predictions. It's designed to respond to big changes in forecaster opinion while still being fairly insensitive to outliers.",
  "faq_community-prediction_content_2": "Here's the mathematical detail:",
  "faq_community-prediction_content_3": "Keep only the most recent prediction from each forecaster.",
  "faq_community-prediction_content_4": "Assign them a number",
  "faq_community-prediction_content_5": ", from oldest to newest (oldest is",
  "faq_community-prediction_content_6": "Weight each by ",
  "faq_community-prediction_content_7": " before being aggregated.",
  "faq_community-prediction_content_8": "For",
  "faq_community-prediction_content_9": "Binary Questions",
  "faq_community-prediction_content_10": ", the Community Prediction is a ",
  "faq_community-prediction_content_11": "weighted median",
  "faq_community-prediction_content_12": " of the individual forecaster probabilities.",
  "faq_community-prediction_content_13": "For",
  "faq_community-prediction_content_14": "Multiple Choice Questions",
  "faq_community-prediction_content_15": ", the Community Prediction is a ",
  "faq_community-prediction_content_16": "weighted median",
  "faq_community-prediction_content_17": " of the individual forecaster probabilities, renormalized to sum to 1 and respect the bounds of ",
  "faq_community-prediction_content_18": "For ",
  "faq_community-prediction_content_19": "Numeric and Date Questions",
  "faq_community-prediction_content_20": ", the Community Prediction is a ",
  "faq_community-prediction_content_21": "weighted average",
  "faq_community-prediction_content_22": " of the individual forecaster distributions.",
  "faq_community-prediction_content_23": "The particular form of the weights means that approximately ",
  "faq_community-prediction_content_24": "forecasters need to predict or update their prediction in order to substantially change the Community Prediction on a question that already has ",
  "faq_community-prediction_content_25": "forecasters.",
  "faq_community-prediction_content_26": "Users can hide the Community Prediction from view from within their settings.",
  "faq_ArebotsincludedintheCommunityPrediction?": "Are bots included in the Community Prediction?",
  "faq_include-bots_content_1": "By default, bots are not included in any aggregations. If they are, it is indicated in the sidebar as \"Include Bots\".",
  "faq_WhatistheMetaculusPrediction?": "What is the Metaculus Prediction?",
  "faq_metaculus-prediction_content_1": "The Metaculus Prediction can only be viewed in the ",
  "faq_metaculus-prediction_content_2": "Aggregation Explorer",
  "faq_metaculus-prediction_content_3": ". It is deprecated since November 2024, but shows a record of the Metaculus system's best estimate of how a question will resolve. It's based on predictions from community members, but unlike the Community Prediction, it's not a simple average or median. Instead, the Metaculus Prediction uses a sophisticated model to calibrate and weight each user, ideally resulting in a prediction that's better than the best of the community.",
  "faq_metaculus-prediction_content_4": "For questions that resolved in 2021, the Metaculus Prediction has a Brier score of 0.107. Lower Brier scores indicate greater accuracy, with the MP slightly lower than the Community Prediction's Brier score of 0.108.",
  "faq_WhycantIseetheCP?": "Why can't I see the CP?",
  "faq_visibility-of-the-cp-and-mp_content_1": "When a question first opens, nobody can see the Community Prediction for a while, to avoid giving inordinate weight to the very first predictions, which may \"ground\" or bias later ones.",
  "faq_WhatArePublicFigurePredictions?": "What Are Public Figure Predictions?",
  "faq_public-figure_content_1": "Public Figure Prediction",
  "faq_public-figure_content_2": " pages are dedicated to collecting and preserving important predictions made by prominent public figures and putting them into conversation with Metaculus community forecasts. Each figure's page features a list of predictions they made along with the source that recorded the prediction, the date the prediction was made, and related Metaculus questions. Public predictions are transparently presented alongside community forecasts in a manner that is inspectable and understandable by all, providing public accountability and additional context for the linked Metaculus questions.&nbsp;",
  "faq_public-figure_content_3": "A",
  "faq_public-figure_content_4": "Public Figure",
  "faq_public-figure_content_5": "is someone with a certain social position within a particular sphere of influence, such as a politician, media personality, scientist, journalist, economist, academic, or business leader.&nbsp;",
  "faq_public-figure_content_6": "What qualifies as a prediction?",
  "faq_public-figure_content_7": "A prediction is a claim or a statement about what someone thinks will happen in the future, where the thing predicted has some amount of uncertainty associated with it.&nbsp;",
  "faq_public-figure_content_8": "A Public Figure Prediction is a prediction made by the public figure themselves and not by figures who might represent them, such as employees, campaign managers, or spokespeople.",
  "faq_public-figure_content_9": "Who can submit Public Figure Predictions?",
  "faq_public-figure_content_10": "When predictions are made by public figures such as elected politicians, public health officials, economists, journalists, and business leaders, they become candidates for inclusion in the Public Figure Prediction system.",
  "faq_public-figure_content_11": "How can I submit a Public Figure Prediction?",
  "faq_public-figure_content_12": "From a Public Figure's page, click Report Prediction and then provide",
  "faq_public-figure_content_13": "A direct quotation from the prediction news source",
  "faq_public-figure_content_14": "The name of the news source",
  "faq_public-figure_content_15": "A link to the news source",
  "faq_public-figure_content_16": "The prediction date",
  "faq_public-figure_content_17": "At least one related Metaculus question",
  "faq_public-figure_content_18": "If the Public Figure does not yet have a dedicated page, you can request that one be created by commenting on the ",
  "faq_public-figure_content_19": "Public Figures Predictions",
  "faq_public-figure_content_20": " discussion post. Tag @christian for a faster moderation process.",
  "faq_public-figure_content_21": "What are the criteria for selecting linked Metaculus questions related to the Public Figure Prediction?",
  "faq_public-figure_content_22": "Depending on the level of specificity and clarity of the Public Figure Prediction, a linked Metaculus question might resolve according to the exact same criteria as the prediction. For example, ",
  "faq_public-figure_content_23": "Joe Biden expressed that he plans to run for reelection",
  "faq_public-figure_content_24": "This Metaculus question asks directly whether he will run",
  "faq_public-figure_content_25": ".&nbsp;&nbsp;",
  "faq_public-figure_content_26": "Linked questions are not required, however, to directly correspond to the public figure's prediction, and ",
  "faq_public-figure_content_27": "this question on whether Biden will be the Democratic nominee in 2024",
  "faq_public-figure_content_28": " is clearly relevant to public figure claim, even as it's further away from the claim than asking whether Biden will run. Relevant linked questions shed light on, create additional context for, or provide potential evidence for or against the public figure's claim. Note that a question being closed or resolved does not disqualify it from being linked to the prediction.",
  "faq_public-figure_content_29": "On the other hand, this question about whether the ",
  "faq_public-figure_content_30": "IRS designates crypto miners as &lsquo;brokers' by 2025",
  "faq_public-figure_content_31": " follows from Biden's Infrastructure Investment and Jobs Act, but beyond the Biden connection, it fails to satisfy the above criteria for a relevant linked question.",
  "faq_public-figure_content_32": "Which sources are acceptable?",
  "faq_public-figure_content_33": "News sources that have authority and are known to be accurate are acceptable. If a number of news sources report the same prediction, but the prediction originated from a single source, using the original source is preferred. Twitter accounts or personal blogs are acceptable if they are owned by the public figure themselves.",
  "faq_public-figure_content_34": "Who decides what happens next?",
  "faq_public-figure_content_35": "Moderators will review and approve your request or provide feedback.",
  "faq_public-figure_content_36": "What happens if a public figure updates their prediction?",
  "faq_public-figure_content_37": "On the page of the prediction, comment the update with the source and tag a moderator. The moderator will review and perform the update if necessary.",
  "faq_public-figure_content_38": "I am the Public Figure who made the prediction. How can I claim this page?",
  "faq_public-figure_content_39": "Please email us at support at metaculus.com.",
  "faq_Whatis\"Reaffirming\"aprediction?": "What is \"Reaffirming\" a prediction?",
  "faq_reaffirming_content_1": "Sometimes you haven't changed your mind on a question, but you still want to record your current forecast. This is called \"reaffirming\": predicting the same value you predicted before, now.",
  "faq_reaffirming_content_2": "It is also useful when sorting questions by the age of your latest forecast. Reaffirming a question sends it to the bottom of that list.",
  "faq_reaffirming_content_3": "You can reaffirm a question from the normal forecast interface on the question page, or using a special button in feeds.",
  "faq_reaffirming_content_4": "On question groups, reaffirming impacts all subquestions on which you had a forecast, but not the others.",
  "faq_ScoresandMedals": "Scores and Medals",
  "faq_Whatarescores?": "What are scores?",
  "faq_whatscores_content_1": "Scores measure forecasting performance over many predictions. Metaculus uses both Baseline scores, which compare you to an impartial baseline, and Peer scores, which compare you to all other forecasters. We also still use Relative scores for tournaments. We do not use the now-obsolete Metaculus points, though they are still computed and you can find them on the question page.",
  "faq_whatscores_content_2": "Learn more in the dedicated ",
  "faq_whatscores_content_3": "Scores FAQ",
  "faq_Whataremedals?": "What are medals?",
  "faq_whatmedals_content_1": "Medals reward Metaculus users for excellence in forecasting accuracy, insightful comment writing, and engaging question writing. We give medals for placing well in any of the 4 leaderboards: Baseline Accuracy, Peer Accuracy, Comments, and Question Writing. Medals are awarded every year. Medals are also awarded for Tournament performance.",
  "faq_whatmedals_content_2": "Learn more in the dedicated ",
  "faq_whatmedals_content_3": "Medals FAQ",
  "faq_MetaculusJournal": "Metaculus Journal",
  "faq_WhatistheMetaculusJournal?": "What is the Metaculus Journal?",
  "faq_whatisjournal_content_1": "The",
  "faq_whatisjournal_content_2": "Metaculus Journal",
  "faq_whatisjournal_content_3": "publishes longform, educational essays on critical topics like emerging science and technology, global health, biosecurity, economics and econometrics, environmental science, and geopolitics—all fortified with testable, quantified forecasts.",
  "faq_whatisjournal_content_4": "If you would like to write for the Metaculus Journal, email ",
  "faq_whatisjournal_content_5": "christian@metaculus.com",
  "faq_whatisjournal_content_6": " with a resume or CV, a writing sample, and two story pitches.",
  "faq_Whatisafortifiedessay?": "What is a fortified essay?",
  "faq_fortifiedessay_content_1": "In November 2021, Metaculus introduced a new project—Fortified Essays. A Fortified Essay is an essay that is “fortified” by its inclusion of quantified forecasts which are justified in the essay. The goal of Fortified Essays is to leverage and demonstrate the knowledge and intellectual labor that went into answering forecasting questions while also putting the forecasts into a larger context.",
  "faq_fortifiedessay_content_2": "Metaculus plans to run Fortified Essay Contests regularly as part of some tournaments. This additional context deriving from essays is necessary, because a quantified forecast in isolation may not provide the information required to drive decision-making by stakeholders. In Fortified Essays, writers can explain the reasoning behind their predictions, discuss the factors driving the predicted outcomes, explore the implications of these outcomes, and offer their own recommendations. By placing forecasts into this larger context, these essays are better able to help stakeholders deeply understand the relevant forecasts and how much weight to place on them. The best essays will be shared with a vibrant and global effective altruism community of thousands of individuals and dozens of organizations.",
  "faq_Miscellany": "Miscellany",
  "faq_WhatareMetaculusProForecasters?": "What are Metaculus Pro Forecasters?",
  "faq_what-are-pros_content_1": "For certain projects, Metaculus employs ",
  "faq_what-are-pros_content_2": "Pro Forecasters",
  "faq_what-are-pros_content_3": "who have demonstrated excellent forecasting ability and who have a history of clearly describing their rationales. Pros forecast on private and public sets of questions to produce well-calibrated forecasts and descriptive rationales for our partners. We primarily recruit members of the Metaculus community with the best track records for our Pro team, but forecasters who have demonstrated excellent forecasting ability elsewhere may be considered as well.",
  "faq_what-are-pros_content_4": "If you’re interested in hiring Metaculus Pro Forecasters for a project, contact us at ",
  "faq_what-are-pros_content_5": "support@metaculus.com",
  "faq_what-are-pros_content_6": " with the subject \"Project Inquiry\".",
  "faq_what-are-pros_content_7": "Metaculus selects individuals according to the following criteria:",
  "faq_what-are-pros_content_8": "Have scores in the top 2% of all Metaculus forecasters.",
  "faq_what-are-pros_content_9": "Have forecasted on a minimum of 75+ questions that have been resolved.",
  "faq_what-are-pros_content_10": "Have experience forecasting for a year or more.",
  "faq_what-are-pros_content_11": "Have forecasted across multiple subject areas.",
  "faq_what-are-pros_content_12": "Have a history of providing commentary explaining their forecasts.",
  "faq_DoesMetaculushaveanAPI?": "Does Metaculus have an API?",
  "faq_api_content_1": "The API documentation is still being worked on.",
  "faq_HowdoIchangemyusername?": "How do I change my username?",
  "faq_change-name_content_1": "You can change your name for free within the first three days of registering. After that, you'll be able to change it once every 180 days.",
  "faq_Imregistered.WhycantIcommentonaquestion?": "I'm registered. Why can't I comment on a question?",
  "faq_cant-comment_content_1": "In an effort to reduce spam, new users must wait 12 hours after signup before commenting is unlocked.",
  "faq_Understandingaccountsuspensions.": "Understanding account suspensions.",
  "faq_suspensions_content_1": "Metaculus may—though this thankfully occurs very rarely—issue the temporary suspensions of an account. This occurs when a user has acted in a way that we consider inappropriate, such as when our ",
  "faq_suspensions_content_2": "terms of use",
  "faq_suspensions_content_3": "are violated. At this point, the user will receive a notice about the suspension and be made aware that continuing this behavior is unacceptable. Temporary suspensions serve as a warning to users that they are a few infractions away from receiving a permanent ban on their account.",
  "faq_WhycanIseetheCommunityPredictiononsomequestions,theMetaculusPredictiononothers,andnopredictiononsomeothers?": "Why can I see the Community Prediction on some questions, the Metaculus Prediction on others, and no prediction on some others?",
  "faq_cant-see_content_1": "When a question first opens, nobody can see the Community Prediction for a while, to avoid giving inordinate weight to the very first predictions, which may \"ground\" or bias later ones. Once the Community Prediction is visible, the Metaculus Prediction is hidden until the question closes.",
  "faq_WhatisNewsMatch?": "What is NewsMatch?",
  "faq_related-news_content_1": "NewsMatch displays a selection of articles relevant to the current Metaculus question. These serve as an additional resource for forecasters as they discuss and predict on the question. Each article is listed with its source and its publication date. Clicking an article title navigates to the article itself. Up and downvoting allows you to indicate whether the article was helpful or not. Your input improves the accuracy and the usefulness of the model that matches articles to Metaculus questions.",
  "faq_related-news_content_2": "The article matching model is supported by ",
  "faq_related-news_content_3": "Improve the News",
  "faq_related-news_content_4": ", a news aggregator developed by a group of researchers at MIT. Designed to give readers more control over their news consumption, Improve the News helps readers stay informed while encountering a wider variety of viewpoints.",
  "faq_related-news_content_5": "Articles in ITN's database are matched with relevant Metaculus questions by a transformer-based machine learning model trained to map semantically similar passages to regions in \"embedding space.\" The embeddings themselves are generated using ",
  "faq_related-news_content_6": "MPNet",
  "faq_WhatareCommunityInsights?": "What are Community Insights?",
  "faq_community-insights_content_1": "Community Insights summarize Metaculus user comments on a given question using GPT-4. They condense recent predictions, timestamped comments, and the current community prediction into concise summaries of relevant arguments for different forecasts on a given question. Forecasters can use them to make more informed decisions and stay up-to-date with the latest insights from the community.",
  "faq_community-insights_content_2": "Community Insights are currently available on binary and continuous questions with large comment threads and will update regularly as new discussion emerges in the comments. If you have feedback on these summaries—or would like to see them appear on a wider variety of questions—email ",
  "faq_community-insights_content_3": "support@metaculus.com",
  "faq_community-insights_content_4": "If you find a Community Insights summary to be incorrect, offensive, or misleading please use the button at the bottom of the summary to \"Flag this summary\" so the Metaculus team can address it.",
  "faq_CanIgetmyownMetaculus?": "Can I get my own Metaculus?",
  "faq_domains_content_1": "Yes! Metaculus has a domain system, where each domain (like \"example.metaculus.com\") has a subset of questions and users that are assigned to it. Each question has a set of domains it is posted on, and each user has a set of domains they are a member of. Thus, a domain is a flexible way of setting a particular set of questions that are private to a set of users, while allowing some questions in the domain to be posted also to metaculus.com. Domains are a product that Metaculus can provide with various levels of support for a fee; please be in touch for more details.",
  "faq_HowcanIhelpspreadthewordaboutMetaculus?": "How can I help spread the word about Metaculus?",
  "faq_spreadword_content_1": "Metaculus will get more fun and more interesting to the extent that it grows to include more and more predictors, so we encourage participants to spread the word to people who they think may enjoy predicting, or just be interested in how the questions develop. Some of the most useful mechanisms are:",
  "faq_spreadword_content_2": "Post particular questions you like to Twitter, Facebook, and Reddit, using the \"share\" button on each page, which sets up a default tweet/post that you can edit.",
  "faq_spreadword_content_3": "Follow us on Twitter",
  "faq_spreadword_content_4": ", then retweet Metaculus tweets to your followers.",
  "faq_spreadword_content_5": "Follow our Facebook page",
  "faq_spreadword_content_6": ", and share posts you like.",
  "faq_spreadword_content_7": "Contact us",
  "faq_spreadword_content_8": "for other ideas.",
  "faq_CanIclosemyMetaculusaccountanddeletemyinformation?": "Can I close my Metaculus account and delete my information?",
  "faq_closeaccount_content_1": "Of course, if you wish to close your account, please email your request to",
  "faq_closeaccount_content_2": "closemyaccount@metaculus.com",
  "faq_closeaccount_content_3": ". Within five business days, we will remove your profile information and comments from our active database.",
  "medals-faq_MedalsFAQ": "Medals FAQ",
  "medals-faq__content_1": "Below are Frequently Asked Questions (and answers!) about medals. The general FAQ is",
  "medals-faq__content_2": "here",
  "medals-faq__content_3": ", and the medals FAQ is ",
  "medals-faq__content_4": "here",
  "medals-faq__content_5": "What are Metaculus medals?",
  "medals-faq__content_6": "What are Baseline Accuracy medals?",
  "medals-faq__content_7": "What are Peer Accuracy medals?",
  "medals-faq__content_8": "What are tournament medals?",
  "medals-faq__content_9": "What are Comments medals?",
  "medals-faq__content_10": "What are Question Writing medals?",
  "medals-faq__content_11": "What is the time period for Comments & Question writing medals?",
  "medals-faq__content_12": "What is the time period for Baseline & Peer medals?",
  "medals-faq__content_13": "What are h-indexes?",
  "medals-faq__content_14": "What is the fractional h-index?",
  "medals-faq__content_15": "How are medal tiers determined?",
  "medals-faq__content_16": "Are medals forever?",
  "medals-faq_WhatareMetaculusmedals?": "What are Metaculus medals?",
  "medals-faq_metaculus-medals_content_1": "Medals reward Metaculus users for excellence in forecasting accuracy, insightful comment writing, and engaging question writing.",
  "medals-faq_metaculus-medals_content_2": "Medals are awarded based on a user's placement in the ",
  "medals-faq_metaculus-medals_content_3": "Leaderboards",
  "medals-faq_metaculus-medals_content_4": ". There are separate leaderboards for each medal category (",
  "medals-faq_metaculus-medals_content_5": "Peer Accuracy",
  "medals-faq_metaculus-medals_content_6": "Baseline Accuracy",
  "medals-faq_metaculus-medals_content_7": "Comments",
  "medals-faq_metaculus-medals_content_8": ", and ",
  "medals-faq_metaculus-medals_content_9": "Question Writing",
  "medals-faq_metaculus-medals_content_10": "), and each leaderboard is further separated into time periods. Medals are also awarded for placement in each ",
  "medals-faq_metaculus-medals_content_11": "Tournament",
  "medals-faq_metaculus-medals_content_12": "'s leaderboard.",
  "medals-faq_metaculus-medals_content_13": "A medal's tier (gold, silver or bronze) is based on a user's rank compared to other users, with only the top 1% earning Gold medals.",
  "medals-faq_metaculus-medals_content_14": "So no one gets an unfair advantage, only public content (questions, comments, tournaments) counts for medals. If you are invited to a private tournament, your activity there will not count toward any medal. We have also decided the three Beginner Tournaments (",
  "medals-faq_metaculus-medals_content_15": ") would not award medals, since that would be unfair to veteran forecasters who were actively discouraged from participating.",
  "medals-faq_metaculus-medals_content_16": "Medals appear in the",
  "medals-faq_metaculus-medals_content_17": "Leaderboards",
  "medals-faq_metaculus-medals_content_18": "and in user profiles.",
  "medals-faq_WhatareBaselineAccuracymedals?": "What are Baseline Accuracy medals?",
  "medals-faq_baseline-medals_content_1": "The Baseline Accuracy medals reward accurate predictions on many questions.",
  "medals-faq_baseline-medals_content_2": "Users are ranked by the sum of their ",
  "medals-faq_baseline-medals_content_3": "Baseline scores",
  "medals-faq_baseline-medals_content_4": "over all questions in the",
  "medals-faq_baseline-medals_content_5": "Time Period",
  "medals-faq_WhatarePeerAccuracymedals?": "What are Peer Accuracy medals?",
  "medals-faq_peer-medals_content_1": "The Peer Accuracy medals reward accurate predictions compared to others, and do not require forecasting a large number of questions.",
  "medals-faq_peer-medals_content_2": "Forecasters are ranked by the sum of their ",
  "medals-faq_peer-medals_content_3": "Peer scores",
  "medals-faq_peer-medals_content_4": ", divided by the sum of their",
  "medals-faq_peer-medals_content_5": "Coverages",
  "medals-faq_peer-medals_content_6": "over all questions in the",
  "medals-faq_peer-medals_content_7": "Time Period",
  "medals-faq_peer-medals_content_8": ". This creates a weighted average, where each prediction is counted proportionally to how long it was standing.",
  "medals-faq_peer-medals_content_9": "If the forecaster has a total coverage below 30 in a particular time period (e.g. they predicted 20 questions with 100% coverage, or 50 questions with 50% coverage), then their coverage is treated as 30. This makes it unlikely that a user wins a medal by getting lucky on a single question.",
  "medals-faq_peer-medals_content_10": "Before 2024, the Peer accuracy was slightly different. The forecaster score was the average of their Peer scores, not taking Coverage into account. This caused some incentives problems, see ",
  "medals-faq_peer-medals_content_11": "here",
  "medals-faq_peer-medals_content_12": " for details. The initial handicap was also 40 instead of the current 30.",
  "medals-faq_Whataretournamentmedals?": "What are tournament medals?",
  "medals-faq_tournament-medals_content_1": "Tournament medals are awarded based on a user's rank on a tournament leaderboard. The top 1% get gold, the next 1% silver, and following 3% bronze.",
  "medals-faq_tournament-medals_content_2": "The three Beginner Tournaments (",
  "medals-faq_tournament-medals_content_3": ") will not award medals, since that would be unfair to veteran forecasters who were actively discouraged from participating.",
  "medals-faq_WhatareCommentsmedals?": "What are Comments medals?",
  "medals-faq_comments-medals_content_1": "A Comments medal is awarded for writing valuable comments, with a balance between quantity and quality.",
  "medals-faq_comments-medals_content_2": "Users are ranked by the ",
  "medals-faq_comments-medals_content_3": "h-index",
  "medals-faq_comments-medals_content_4": "of upvotes on their comments made during the ",
  "medals-faq_comments-medals_content_5": "Time Period",
  "medals-faq_WhatareQuestionWritingmedals?": "What are Question Writing medals?",
  "medals-faq_question-writing-medals_content_1": "The Question Writing medals reward writing engaging questions, with a balance between quantity and quality.",
  "medals-faq_question-writing-medals_content_2": "Users are ranked by the ",
  "medals-faq_question-writing-medals_content_3": "h-index",
  "medals-faq_question-writing-medals_content_4": "of the number of forecasters who predicted on their authored questions in the ",
  "medals-faq_question-writing-medals_content_5": "Time Period",
  "medals-faq_question-writing-medals_content_6": ". Because there are few questions but many forecasters, the number of forecasters is divided by 10 before being used in the h-index.",
  "medals-faq_question-writing-medals_content_7": "All co-authors on a question receive full credit, i.e. they are treated the same as if they had authored the question alone.",
  "medals-faq_question-writing-medals_content_8": "Additionally, a single question may contribute to medals over many years, not just the year it was written. If a question receives predictions from 200 unique forecasters every year, then the author receives credit for those 200 forecasters every year.",
  "medals-faq_WhataretheTimesPeriodsforComments&Questionwritingmedals?": "What are the Times Periods for Comments & Question writing medals?",
  "medals-faq_writing-time-periods_content_1": "Comments and Question writing medals are awarded annually, based on the # of upvotes on your comments and # of forecasters on your questions in a given calendar year.",
  "medals-faq_writing-time-periods_content_2": "For example, if you wrote 20 long-term questions in 2016 that each attracted 200 forecasters in every calendar year then your score for Question Writing would be 20 in every year after 2016. Even though you didn't write any questions in 2017, the engagement that your questions attracted in 2017 makes you eligible for a 2017 medal. Said another way, a great long-term question can contribute to many medals.",
  "medals-faq_WhataretheTimePeriodsforBaseline&Peermedals?": "What are the Time Periods for Baseline & Peer medals?",
  "medals-faq_scores-time-periods_content_1": "Time Periods for Accuracy medals serve two main purposes. They ensure a periodic fair starting line on January 1, at which point long-time and new forecasters are on equal grounds. They also group questions with similar durations together, so it is easier to separate long-term and short-term forecasting skill.",
  "medals-faq_scores-time-periods_content_2": "A Time Period for the Baseline and Peer medals consists of a Duration (1, 2, 5, 10… years), a start year and an end year. The end date for a time period is December 31 of the end year. The start date for a time period is January 1 of the start year. So, a 5 year medal covering 2016–2020 has a start date of Jan 1, 2016 and an end date of Dec 31, 2020.",
  "medals-faq_scores-time-periods_content_3": "The Time Period determines which questions are included in a medal calculation:",
  "medals-faq_scores-time-periods_content_4": "A question only belongs to exactly 1 Time Period, so it only contributes to 1 Baseline medal and to 1 Peer medal.",
  "medals-faq_scores-time-periods_content_5": "A question is assigned to the shortest time period that satisfies the following:",
  "medals-faq_scores-time-periods_content_6": "The question opened after the Time Period start date.",
  "medals-faq_scores-time-periods_content_7": "The question is",
  "medals-faq_scores-time-periods_content_8": "scheduled",
  "medals-faq_scores-time-periods_content_9": "to close",
  "medals-faq_scores-time-periods_content_10": "before",
  "medals-faq_scores-time-periods_content_11": "the Time Period's end date. ",
  "medals-faq_scores-time-periods_content_12": "(There is a 3 day buffer here: some questions were written to close on Jan 1 and naturally belong in the prior year. Also, sometimes time zones make it unclear which date to use.)",
  "medals-faq_scores-time-periods_content_13": "The question resolves before the Time Period's end date + a buffer of 100 days. ",
  "medals-faq_scores-time-periods_content_14": "(This allows time for data sources to become available. For instance, a question about 2022 GDP naturally should count toward the 2022 medal, but the final economic report is often published ~90 days after the year end.)",
  "medals-faq_scores-time-periods_content_15": "Following the rules above, almost all questions are assigned to their Time Period before forecasting begins. On rare occasions, a question will fail to be resolved before the end of the 100-day buffer: by the rules above it is automatically assigned to the next higher Duration in which it fits.",
  "medals-faq_scores-time-periods_content_16": "Note: If a question closes early it remains in its originally assigned time period. This is important to ensure that an optimistic forecaster does not gain an advantage. For example, imagine ten 5-year questions that can either resolve Yes this week (with 50% probability), or resolve No after the full 5 years. Starting next week, an optimist looks misleadingly good: they predicted 99% on all of the 5 questions that resolved, and they all resolved Yes. After the full 5 years they correctly look very bad: of the 10 questions they predicted 99% on, only 5 resolved Yes. Keeping questions in their initial time period ensures that optimists don't get undue early credit.",
  "medals-faq_Whatareh-indexes?": "What are h-indexes?",
  "medals-faq_h-indexes_content_1": "An",
  "medals-faq_h-indexes_content_2": "h-index",
  "medals-faq_h-indexes_content_3": "is a metric commonly used in academia to measure the quantity and quality of researcher publications. If a researcher has an h-index of N it means that they have published at least N papers that each individually have at least N citations.",
  "medals-faq_h-indexes_content_4": "We use h-indexes for the Comments medals (number of upvotes per comment) and for the Question Writing medals (tens of forecasters per question).",
  "medals-faq_h-indexes_content_5": "Traditional h-indexes are integers. To break ties, we use a fractional h-index, described below.",
  "medals-faq_Whatisthefractionalh-index?": "What is the fractional h-index?",
  "medals-faq_fractional-h-index_content_1": "The fractional h-index is like the standard h-index, but with an added fractional part that measures progress toward the next higher h-index value.",
  "medals-faq_fractional-h-index_content_2": "Imagine that you have exactly 2 comments with exactly 2 upvotes. Your h-index is therefore 2. To reach an h-index of 3, you need to receive 1 more upvote on each of your 2 existing comments (a total of 2 more upvotes) and you need to write a new comment that receives 3 upvotes. In total you need 5 more upvotes to reach an h-index of 3.",
  "medals-faq_fractional-h-index_content_3": "Imagine one of your comments receives 1 new upvote, and you write a comment that receives 2 new upvotes. Your fractional h-index is then:",
  "medals-faq_fractional-h-index_content_4": "In general, the formula is:",
  "medals-faq_fractional-h-index_content_5": "Where",
  "medals-faq_fractional-h-index_content_6": "is your integer h-index, and ",
  "medals-faq_fractional-h-index_content_7": "is the number of upvotes on your i-th most upvoted comment.",
  "medals-faq_Howaremedaltiersdetermined?": "How are medal tiers determined?",
  "medals-faq_medal-tiers_content_1": "Medals are awarded based on a user's rank within a Category for a Time Period, or in a Tournament:",
  "medals-faq_medal-tiers_content_2": "Top 1% = Gold",
  "medals-faq_medal-tiers_content_3": "1% to 2% = Silver",
  "medals-faq_medal-tiers_content_4": "2% to 5% = Bronze",
  "medals-faq_medal-tiers_content_5": "The denominators for the percentages are the number of users who have made any contribution toward that medal. Specifically, the denominators are:",
  "medals-faq_medal-tiers_content_6": "Baseline & Peer Accuracy: the number of users who made a forecast on any public question in the time period.",
  "medals-faq_medal-tiers_content_7": "Tournament: the number of users who made a forecast on any question in the tournament.",
  "medals-faq_medal-tiers_content_8": "Comments: the number of users who made a public comment in the time period.",
  "medals-faq_medal-tiers_content_9": "Question writing: the number of users who authored (or co-authored) a public question that received forecasts during the time period.",
  "medals-faq_medal-tiers_content_10": "To make the leaderboards more interesting and fair, we also enforce the following rules:",
  "medals-faq_medal-tiers_content_11": "The first, second, and third place finishers always receive (at least) a gold, silver, and bronze medals, in that order.",
  "medals-faq_medal-tiers_content_12": "If two users are tied, they always get the same medal and rank (e.g. if 2nd and 3rd tie, they both get 2nd place).",
  "medals-faq_medal-tiers_content_13": "Metaculus staff and moderators are ineligible for medals for contributions they made during the time they were on staff or moderating.",
  "medals-faq_Aremedalsforever?": "Are medals forever?",
  "medals-faq_medals-forever_content_1": "In general, yes! We designed the medal system so that once a medal is awarded, it never goes away.",
  "medals-faq_medals-forever_content_2": "However, when we discover an error - an incorrectly resolved question or a bug in the code - we plan to correct the error and medals could shift, hopefully only very slightly. We believe this will be a rare occurrence, but it may happen. The spirit of Metaculus is to be accurate.",
  "scores-faq_ScoresFAQ": "Scores FAQ",
  "scores-faq__content_1": "Below are Frequently Asked Questions (and answers!) about scores. The general FAQ is",
  "scores-faq__content_2": "here",
  "scores-faq__content_3": ", and the medals FAQ is ",
  "scores-faq__content_4": "here",
  "scores-faq__content_5": "Scores",
  "scores-faq__content_6": "What is a scoring rule?",
  "scores-faq__content_7": "What is a proper scoring rule?",
  "scores-faq__content_8": "What is the log score?",
  "scores-faq__content_9": "What is the log score for continuous questions?",
  "scores-faq__content_10": "What is a spot score?",
  "scores-faq__content_11": "What is the Baseline score?",
  "scores-faq__content_12": "What is the Peer score?",
  "scores-faq__content_13": "Why is the Peer Score of the Community Prediction positive?",
  "scores-faq__content_14": "Do all my predictions on a question count toward my score?",
  "scores-faq__content_15": "Can I get better scores by predicting extreme values?",
  "scores-faq__content_16": "Why did I get a small score when I was right?",
  "scores-faq__content_17": "What are the legacy scores?",
  "scores-faq__content_18": "What is the Relative score?",
  "scores-faq__content_19": "What is the coverage?",
  "scores-faq__content_20": "What are Metaculus points?",
  "scores-faq__content_21": "Tournaments",
  "scores-faq__content_22": "How are my tournament Score, Coverage, Take, Prize and Rank calculated?",
  "scores-faq__content_23": "How are my (legacy) tournament Score, Coverage, Take, Prize and Rank calculated?",
  "scores-faq__content_24": "What are the Hidden Period and Hidden Coverage Weights?",
  "scores-faq_Scores": "Scores",
  "scores-faq_Whatisascoringrule?": "What is a scoring rule?",
  "scores-faq_scoring-rule_content_1": "A scoring rule is a mathematical function which, given a prediction and an outcome, gives a score in the form of a number.",
  "scores-faq_scoring-rule_content_2": "A naive scoring rule could be: \"you score equals the probability you gave to the correct outcome\". So, for example, if you predict 80% and the question resolves Yes, your score would be 0.8 (and 0.2 if the question resolved No). At first glance this seems like a good scoring rule: forecasters who gave predictions closer to the truth get higher scores.",
  "scores-faq_scoring-rule_content_3": "Unfortunately this scoring rule is not \"proper\", as we'll see in the next section.",
  "scores-faq_Whatisaproperscoringrule?": "What is a proper scoring rule?",
  "scores-faq_proper-scoring_content_1": "Proper scoring rules have a very special property: the only way to optimize your score on average is to predict your sincere beliefs.",
  "scores-faq_proper-scoring_content_2": "How do we know that the naive scoring rule from the previous section is not proper? An example should be illuminating: consider the question \"Will I roll a 6 on this fair die?\". Since the die is fair, your belief is \"1/6\" or about 17%. Now consider three possibilities: you could either predict your true belief (17%), predict something more extreme, like 5%, or predict something less extreme, like 30%. Here's a table of the scores you expect for each possible die roll:",
  "scores-faq_proper-scoring_content_3": "outcome die roll",
  "scores-faq_proper-scoring_content_4": "naive score of p=5%",
  "scores-faq_proper-scoring_content_5": "naive score of p=17%",
  "scores-faq_proper-scoring_content_6": "naive score of p=30%",
  "scores-faq_proper-scoring_content_7": "average",
  "scores-faq_proper-scoring_content_8": "Which means you get a better score on average if you predict 5% than 17%. In other words, this naive score incentivizes you to predict something other than the true probability. This is very bad!",
  "scores-faq_proper-scoring_content_9": "Proper scoring rules do not have this problem: your score is best when you predict the true probability. The log score, which underpins all Metaculus scores, is a proper score (see ",
  "scores-faq_proper-scoring_content_10": "What is the log score?",
  "scores-faq_proper-scoring_content_11": "). We can compare the scores you get in the previous example:",
  "scores-faq_proper-scoring_content_12": "outcome die roll",
  "scores-faq_proper-scoring_content_13": "log score of p=5%",
  "scores-faq_proper-scoring_content_14": "log score of p=17%",
  "scores-faq_proper-scoring_content_15": "log score of p=30%",
  "scores-faq_proper-scoring_content_16": "average",
  "scores-faq_proper-scoring_content_17": "With the log score, you do get a higher (better) score if you predict the true probability of 17%.",
  "scores-faq_Whatisthelogscore?": "What is the log score?",
  "scores-faq_log-score_content_1": "The logarithmic scoring rule, or \"log score\" for short, is defined as:",
  "scores-faq_log-score_content_2": "Where",
  "scores-faq_log-score_content_3": "is the natural logarithm and",
  "scores-faq_log-score_content_4": "is the probability predicted for the outcome that actually happened. This log score applies to categorical predictions, where one of a (usually) small set of outcomes can happen. On Metaculus those are Binary and Multiple Choice questions. See the next section for the log scores of continuous questions.",
  "scores-faq_log-score_content_5": "Higher scores are better:",
  "scores-faq_log-score_content_6": "If you predicted 0% on the correct outcome, your score will be ",
  "scores-faq_log-score_content_7": "(minus infinity).",
  "scores-faq_log-score_content_8": "If you predict 100% on the correct outcome, your score will be 0.",
  "scores-faq_log-score_content_9": "This means that the log score is always negative (for Binary and Multiple Choice questions). This has proved unintuitive, which is one reason why Metaculus uses the ",
  "scores-faq_log-score_content_10": "Baseline",
  "scores-faq_log-score_content_11": "and ",
  "scores-faq_log-score_content_12": "Peer",
  "scores-faq_log-score_content_13": "scores, which are based on the log score but can be positive.",
  "scores-faq_log-score_content_14": "The log score is proper (see ",
  "scores-faq_log-score_content_15": "What is a proper scoring rule?",
  "scores-faq_log-score_content_16": "). This means that to maximize your score ",
  "scores-faq_log-score_content_17": "you should predict your true beliefs",
  "scores-faq_log-score_content_18": "(see ",
  "scores-faq_log-score_content_19": "Can I get better scores by predicting extreme values?",
  "scores-faq_log-score_content_20": "One interesting property of the log score: it is much more punitive of extreme wrong predictions than it is rewarding of extreme right predictions. Consider the scores you get for predicting 99% or 99.9%:",
  "scores-faq_log-score_content_21": "99% Yes, 1% No",
  "scores-faq_log-score_content_22": "99.9% Yes, 0.1% No",
  "scores-faq_log-score_content_23": "Score if outcome = Yes",
  "scores-faq_log-score_content_24": "Score if outcome = No",
  "scores-faq_log-score_content_25": "Going from 99% to 99.9% only gives you a tiny advantage if you are correct (+0.009), but a huge penalty if you are wrong (-2.3). So be careful, and only use extreme probabilities when you're sure they're appropriate!",
  "scores-faq_Whatisthelogscoreforcontinuousquestions?": "What is the log score for continuous questions?",
  "scores-faq_continuous-log-score_content_1": "Since the domain of possible outcomes for continuous questions is (drum roll) continuous, any outcome has mathematically 0 chance of happening. Thankfully we can adapt the log score in the form:",
  "scores-faq_continuous-log-score_content_2": "Where",
  "scores-faq_continuous-log-score_content_3": "is the natural logarithm and",
  "scores-faq_continuous-log-score_content_4": "is the value of the predicted ",
  "scores-faq_continuous-log-score_content_5": "probability density function",
  "scores-faq_continuous-log-score_content_6": " at the outcome. Note that on Metaculus, all pdfs have a ",
  "scores-faq_continuous-log-score_content_7": "uniform distribution",
  "scores-faq_continuous-log-score_content_8": " of height 0.01 added to them. This prevents extreme log scores.",
  "scores-faq_continuous-log-score_content_9": "This is also a proper scoring rule, and behaves in somewhat similar ways to the log score described above. One difference is that, contrary to probabilities that are always between 0 and 1, ",
  "scores-faq_continuous-log-score_content_10": "values can be greater than 1. This means that the continuous log score can be greater than 0: in theory it has no maximum value, but in practice Metaculus restricts how sharp pdfs can get (see the maximum scores tabulated below).",
  "scores-faq_Whatisaspotscore?": "What is a spot score?",
  "scores-faq_spot-score_content_1": "A \"spot\" score is a specific version of the given score type (e.g. \"spot peer score\") where the evaluation doesn't take prediction duration into account. For a spot score, only the prediction at a specified time is considered. Unless otherwise indicated, spot scores are evaluated at the same time the Community Prediction is revealed. Coverage is 100% if there is an active prediction at the time, and 0% if there is not. The math is the same as the given score type.",
  "scores-faq_WhatistheBaselinescore?": "What is the Baseline score?",
  "scores-faq_baseline-score_content_1": "The Baseline score compares a prediction to a fixed \"chance\" baseline. If it is positive, the prediction was better than chance. If it is negative, it was worse than chance.",
  "scores-faq_baseline-score_content_2": "That \"chance\" baseline gives the same probability to all outcomes. For binary questions, this is a prediction of 50%. For an N-option multiple choice question it is a prediction of 1/N for every option. For continuous questions this is a uniform (flat) distribution.",
  "scores-faq_baseline-score_content_3": "The Baseline score is derived from the ",
  "scores-faq_baseline-score_content_4": "log score",
  "scores-faq_baseline-score_content_5": ", rescaled so that:",
  "scores-faq_baseline-score_content_6": "Predicting the same probability on all outcomes gives a score of 0.",
  "scores-faq_baseline-score_content_7": "Predicting perfectly on a binary or multiple choice question gives a score of +100.",
  "scores-faq_baseline-score_content_8": "The average scores of binary and continuous questions roughly match.",
  "scores-faq_baseline-score_content_9": "Here are some notable values for the Baseline score:",
  "scores-faq_baseline-score_content_10": "Binary questions",
  "scores-faq_baseline-score_content_11": "Multiple Choice questions",
  "scores-faq_baseline-score_content_12": "(8 options)",
  "scores-faq_baseline-score_content_13": "Continuous questions",
  "scores-faq_baseline-score_content_14": "Best possible Baseline score on Metaculus",
  "scores-faq_baseline-score_content_15": "Worst possible Baseline score on Metaculus",
  "scores-faq_baseline-score_content_16": "Median Baseline empirical score",
  "scores-faq_baseline-score_content_17": "no data yet",
  "scores-faq_baseline-score_content_18": "Average Baseline empirical score",
  "scores-faq_baseline-score_content_19": "no data yet",
  "scores-faq_baseline-score_content_20": "Theoretically, binary scores can be infinitely negative, and continuous scores can be both infinitely positive and infinitely negative. In practice, Metaculus restricts binary predictions to be between 0.1% and 99.9%, and continuous pdfs to be between 0.01 and ~35, leading to the scores above. The empirical scores are based on all scores observed on all resolved Metaculus questions, as of November 2023.",
  "scores-faq_baseline-score_content_21": "Note that the above describes the Baseline score at a single point in time. Metaculus scores are time-averaged over the lifetime of the question, see ",
  "scores-faq_baseline-score_content_22": "Do all my predictions on a question count toward my score?",
  "scores-faq_baseline-score_content_23": "You can expand the section below for more details and maths.",
  "scores-faq_WhatisthePeerscore?": "What is the Peer score?",
  "scores-faq_peer-score_content_1": "The Peer score compares a prediction to all the other predictions made on the same question. If it is positive, the prediction was (on average) better than others. If it is negative it was worse than others.",
  "scores-faq_peer-score_content_2": "The Peer score is derived from the ",
  "scores-faq_peer-score_content_3": "log score",
  "scores-faq_peer-score_content_4": ": it is the average difference between a prediction's log score, and the log scores of all other predictions on that question. Like the Baseline score, the Peer score is multiplied by 100.",
  "scores-faq_peer-score_content_5": "One interesting property of the Peer score is that, on any given question, the sum of all participants' Peer scores is always 0. This is because each forecaster's score is their average difference with every other: when you add all the scores, all the differences cancel out and the result is 0. Here's a quick example: imagine a ",
  "scores-faq_peer-score_content_6": "continuous question",
  "scores-faq_peer-score_content_7": ", with three forecasters having predicted:",
  "scores-faq_peer-score_content_8": "Forecaster",
  "scores-faq_peer-score_content_9": "log score",
  "scores-faq_peer-score_content_10": "Peer score",
  "scores-faq_peer-score_content_11": "Alex",
  "scores-faq_peer-score_content_12": "Bailey",
  "scores-faq_peer-score_content_13": "Cory",
  "scores-faq_peer-score_content_14": "sum",
  "scores-faq_peer-score_content_15": "Here are some notable values for the Peer score:",
  "scores-faq_peer-score_content_16": "Binary and",
  "scores-faq_peer-score_content_17": "Multiple Choice",
  "scores-faq_peer-score_content_18": "questions",
  "scores-faq_peer-score_content_19": "Continuous",
  "scores-faq_peer-score_content_20": "questions",
  "scores-faq_peer-score_content_21": "Best possible Peer score on Metaculus",
  "scores-faq_peer-score_content_22": "Worst possible Peer score on Metaculus",
  "scores-faq_peer-score_content_23": "Median Peer empirical score",
  "scores-faq_peer-score_content_24": "Average Peer empirical score",
  "scores-faq_peer-score_content_25": "*The average Peer score is 0 by definition.",
  "scores-faq_peer-score_content_26": "Theoretically, binary scores can be infinitely negative, and continuous scores can be both infinitely positive and infinitely negative. In practice, Metaculus restricts binary predictions to be between 0.1% and 99.9%, and continuous pdfs to be between 0.01 and ~35, leading to the scores above.",
  "scores-faq_peer-score_content_27": "The \"empirical scores\" are based on all scores observed on all resolved Metaculus questions, as of November 2023.",
  "scores-faq_peer-score_content_28": "Note that the above describes the Peer score at a single point in time. Metaculus scores are time-averaged over the lifetime of the question, see ",
  "scores-faq_peer-score_content_29": "Do all my predictions on a question count toward my score?",
  "scores-faq_peer-score_content_30": "You can expand the section below for more details and maths.",
  "scores-faq_WhyisthePeerscoreoftheCommunityPredictionpositive?": "Why is the Peer score of the Community Prediction positive?",
  "scores-faq_cp-positive-peer_content_1": "The",
  "scores-faq_cp-positive-peer_content_2": "Peer score",
  "scores-faq_cp-positive-peer_content_3": "measures whether a forecaster was on average better than other forecasters. It is the difference between the forecaster's ",
  "scores-faq_cp-positive-peer_content_4": "log score",
  "scores-faq_cp-positive-peer_content_5": "and the average of all other forecasters' log scores. If you have a positive Peer score, it means your log score was better than the average of all other forecasters' log scores.",
  "scores-faq_cp-positive-peer_content_6": "The",
  "scores-faq_cp-positive-peer_content_7": "Community Prediction",
  "scores-faq_cp-positive-peer_content_8": "is a time-weighted median of all forecasters on the question. Like most aggregates, it is better than most of the forecasters it feeds on: it is less noisy, less biased, and updates more often.",
  "scores-faq_cp-positive-peer_content_9": "Since the Community Prediction is better than most forecasters, it follows that its score should be higher than the average score of all forecasters. And so its Peer score is positive.",
  "scores-faq_Doallmypredictionsonaquestioncounttowardmyscore?": "Do all my predictions on a question count toward my score?",
  "scores-faq_time-averaging_content_1": "Yes. Metaculus uses time-averaged scores, so all your predictions count, proportional to how long they were standing. An example goes a long way (we will use the Baseline score for simplicity, but the same logic applies to any score):",
  "scores-faq_time-averaging_content_2": "A binary question is open 5 days, then closes and resolves Yes. You start predicting on the second day, make these predictions, and get those scores:",
  "scores-faq_time-averaging_content_3": "Day 1",
  "scores-faq_time-averaging_content_4": "Day 2",
  "scores-faq_time-averaging_content_5": "Day 3",
  "scores-faq_time-averaging_content_6": "Day 4",
  "scores-faq_time-averaging_content_7": "Day 5",
  "scores-faq_time-averaging_content_8": "Average",
  "scores-faq_time-averaging_content_9": "Prediction",
  "scores-faq_time-averaging_content_10": "N/A",
  "scores-faq_time-averaging_content_11": "Baseline score",
  "scores-faq_time-averaging_content_12": "Some things to note:",
  "scores-faq_time-averaging_content_13": "Before you predict, your score is considered to be 0 (this is true for all scores based on the log score). This means that if you believe you can do better than 0, you should predict as early as possible.",
  "scores-faq_time-averaging_content_14": "You have a score for Day 4, despite not having predicted that day. This is because your predictions stay standing until you update them, so on Day 4 you were scored on your Day 3 prediction. On Day 5 you updated to 80%, so you were scored on that.",
  "scores-faq_time-averaging_content_15": "This example uses days, but your Metaculus scores are based on exact timestamped predictions, so a prediction left standing for 1 hour will count for 1/24th of a prediction left standing for a day, etc.",
  "scores-faq_time-averaging_content_16": "Lastly, note that scores are always averaged for every instant between the Open date and (scheduled) Close date of the question. If a question resolves early (i.e. before the scheduled close date), then scores are set to 0 between the resolution date and scheduled close date, and still count in the average. This ensures alignment of incentives, as explained in the section ",
  "scores-faq_time-averaging_content_17": "Why did I get a small score when I was right?",
  "scores-faq_time-averaging_content_18": " below.",
  "scores-faq_CanIgetbetterscoresbypredictingextremevalues?": "Can I get better scores by predicting extreme values?",
  "scores-faq_extremizing_content_1": "Metaculus uses proper scores (see ",
  "scores-faq_extremizing_content_2": "What is a proper scoring rule?",
  "scores-faq_extremizing_content_3": "), so you cannot get a better score (on average) by making predictions more extreme than your beliefs. On any question, if you want to maximize your expected score, you should predict exactly what you believe.",
  "scores-faq_extremizing_content_4": "Let's walk through a simple example using the Baseline score. Suppose you are considering predicting a binary question. After some thought, you conclude that the question has 80% chance to resolve Yes.",
  "scores-faq_extremizing_content_5": "If you predict 80%, you will get a score of +68 if the question resolves Yes, and -132 if it resolves No. Since you think there is an 80% chance it resolves Yes, you expect on average a score of",
  "scores-faq_extremizing_content_6": "If you predict 90%, you will get a score of +85 if the question resolves Yes, and -232 if it resolves No. Since you think there is an 80% chance it resolves Yes, you expect on average a score of",
  "scores-faq_extremizing_content_7": "So by predicting a more extreme value, you actually lower the score you expect to get (on average!).",
  "scores-faq_extremizing_content_8": "Here are some more values from the same example, tabulated:",
  "scores-faq_extremizing_content_9": "Prediction",
  "scores-faq_extremizing_content_10": "Score if Yes",
  "scores-faq_extremizing_content_11": "Score if No",
  "scores-faq_extremizing_content_12": "Expected score",
  "scores-faq_extremizing_content_13": "The 99% prediction gets the highest score when the question resolves Yes, but it also gets the lowest score when it resolves No. This is why, on average, the strategy that maximizes your score is to predict what you believe. This is one of the reasons why looking at scores on individual questions is not very informative; only aggregate over many questions are interesting!",
  "scores-faq_WhydidIgetasmallscorewhenIwasright?": "Why did I get a small score when I was right?",
  "scores-faq_score-truncation_content_1": "To make sure incentives are aligned, Metaculus needs to ensure that our scores are proper. We also time-average scores.",
  "scores-faq_score-truncation_content_2": "This has a counter-intuitive consequence: when a question resolves before its intended close date, the times between resolution and close date need to count in the time-average, with scores of 0. We call this \"score truncation\".",
  "scores-faq_score-truncation_content_3": "An example is best: imagine the question \"Will a new human land on the Moon before 2030?\". It can either resolve Yes before 2030 (because someone landed on the Moon), or it can resolve No in 2030. If we did not truncate scores, you could game this question by predicting close to 100% in the beginning (since it can only resolve positive early), and lower later (since it can only resolve negative at the end).",
  "scores-faq_score-truncation_content_4": "Another way to think about this is that if a question lasts a year, then each day (or in fact each second) is scored as a separate question. To preserve properness, it is imperative that each day is weighted the same in the final average (or at least that the weights be decided in advance). From this perspective, not doing truncation is equivalent to retroactively giving much more weight to days before the question resolves, which is not proper.",
  "scores-faq_score-truncation_content_5": "You can read a worked example with maths by expanding the section below.",
  "scores-faq_Whatarethelegacyscores?": "What are the legacy scores?",
  "scores-faq_WhatistheRelativescore?": "What is the Relative score?",
  "scores-faq_relative-score_content_1": "The Relative score compares a prediction to the median of all other predictions on the same question. If it is positive, the prediction was (on average) better than the median. If it is negative it was worse than the median.",
  "scores-faq_relative-score_content_2": "It is based on the log score, with the formula:",
  "scores-faq_relative-score_content_3": "Where",
  "scores-faq_relative-score_content_4": "is the prediction being scored and",
  "scores-faq_relative-score_content_5": "is the median of all other predictions on that question.",
  "scores-faq_relative-score_content_6": "As of late 2023, the Relative score is in the process of being replaced by the",
  "scores-faq_relative-score_content_7": "Peer score",
  "scores-faq_relative-score_content_8": ", but it is still used for many open tournaments.",
  "scores-faq_Whatisthecoverage?": "What is the coverage?",
  "scores-faq_coverage_content_1": "The Coverage measures for what proportion of a question's lifetime you had a prediction standing.",
  "scores-faq_coverage_content_2": "If you make your first prediction right when the question opens, your coverage will be 100%. If you make your first prediction one second before the question closes, your coverage will be very close to 0%.",
  "scores-faq_coverage_content_3": "The Coverage is used in tournaments, to incentivize early predictions.",
  "scores-faq_WhatareMetaculuspoints?": "What are Metaculus points?",
  "scores-faq_old-points_content_1": "Metaculus points were used as the main score on Metaculus until late 2023.",
  "scores-faq_old-points_content_2": "You can still find the rankings based on points ",
  "scores-faq_old-points_content_3": "here",
  "scores-faq_old-points_content_4": "They are a proper score, based on the log score. They are a mixture of a Baseline-like score and a Peer-like score, so they reward both beating an impartial baseline and beating other forecasters.",
  "scores-faq_old-points_content_5": "For full mathematical details, expand the section below.",
  "scores-faq_Tournaments": "Tournaments",
  "scores-faq_HowaremytournamentScore,Take,Prize,andRankcalculated?": "How are my tournament Score, Take, Prize, and Rank calculated?",
  "scores-faq_tournament-scores_content_1": "This scoring method was introduced in March 2024. It is based on the ",
  "scores-faq_tournament-scores_content_2": "Peer scores",
  "scores-faq_tournament-scores_content_3": "described above.",
  "scores-faq_tournament-scores_content_4": "Your rank in the tournament is determined by the sum of your Peer scores over all questions weighted by the question's weight in the tournament (you get 0 for any question you didn’t forecast). Questions that have weights other than 1.0 are indicated in the sidebar of the question detail page. Typically, a question weight is changed if it is determined to be highly correllated with other questions included in the same tournament, especially question groups.",
  "scores-faq_tournament-scores_content_5": "The share of the prize pool you get is proportional to that same sum of Peer scores, squared. If the sum of your Peer scores is negative, you don’t get any prize.",
  "scores-faq_tournament-scores_content_6": "For a tournament with a sufficiently large number of independent questions, this scoring method is effectively ",
  "scores-faq_tournament-scores_content_7": "proper",
  "scores-faq_tournament-scores_content_8": " for the top quartile of forecasters. While there are small imperfections for forecasters near a 0 Peer score for which they might win a tiny bit of money by extremizing their forecasts, we believe this is an edge case that you can safely ignore. In short, you should predict your true belief on any question.",
  "scores-faq_tournament-scores_content_9": "Taking the square of your Peer scores incentivizes forecasting every question and forecasting early. Don’t forget to",
  "scores-faq_tournament-scores_content_10": "Follow",
  "scores-faq_tournament-scores_content_11": "a tournament to be notified of new questions.",
  "scores-faq_Howaremy(legacy)tournamentScore,Coverage,Take,Prize,andRankcalculated?": "How are my (legacy) tournament Score, Coverage, Take, Prize, and Rank calculated?",
  "scores-faq_legacy-tournament-scores_content_1": "This scoring method was superseded in March 2024 by the New Tournament Score described above. It is still in use for tournaments that concluded before March 2024 for some tournaments that were in flight then.",
  "scores-faq_legacy-tournament-scores_content_2": "Your tournament Score is the sum of your Relative scores over all questions in the tournament. If, on average, you were better than the Community Prediction, then it will be positive; otherwise, it will be negative.",
  "scores-faq_legacy-tournament-scores_content_3": "Your tournament Coverage is the average of your coverage on each question. If you predicted all questions when they opened, your Coverage will be 100%. If you predicted all questions halfway through, or if you predicted half the questions when they opened, your Coverage will be 50%.",
  "scores-faq_legacy-tournament-scores_content_4": "Your tournament Take is the exponential of your Score, times your Coverage:",
  "scores-faq_legacy-tournament-scores_content_5": "Your Prize is how much money you earned on that tournament. It is proportional to your take and is equal to your Take divided by the sum of all competing forecasters' Takes.",
  "scores-faq_legacy-tournament-scores_content_6": "Your Rank is simply how high you were in the leaderboard, sorted by Prize.",
  "scores-faq_legacy-tournament-scores_content_7": "The higher your Score and Coverage, the higher your Take will be. The higher your Take, the more Prize you'll receive, and the higher your Rank will be.",
  "scores-faq_WhataretheHiddenPeriodandHiddenCoverageWeights?": "What are the Hidden Period and Hidden Coverage Weights?",
  "scores-faq_hidden-period_content_1": "The Community Prediction is on average much better than most forecasters. This means that you could get decent scores by just copying the Community Prediction at all times. To prevent this, many tournament questions have a significant period of time at the beginning when the Community Prediction is hidden. We call this time the Hidden Period.",
  "scores-faq_hidden-period_content_2": "To incentivize forecasting during the hidden period, questions sometimes are also set up so that the coverage you accrue during the Hidden Period counts more. For example, the Hidden Period could count for 50% of the question coverage, or even 100%. We call this percentage the Hidden Period Coverage Weight.",
  "scores-faq_hidden-period_content_3": "If the Hidden Period Coverage Weight is 50%, then if you don't forecast during the hidden period your coverage will be at most 50%, regardless of how long the hidden period lasted.",
  "scores-faq-baseline-math__content_1": "The Baseline scores are rescaled ",
  "scores-faq-baseline-math__content_2": "log scores",
  "scores-faq-baseline-math__content_3": ", with the general form:",
  "scores-faq-baseline-math__content_4": "For binary and multiple choice questions, the ",
  "scores-faq-baseline-math__content_5": "is chosen so that a perfect prediction (",
  "scores-faq-baseline-math__content_6": ") gives a score of +100. The formula for a binary question is:",
  "scores-faq-baseline-math__content_7": "Note that you can rearrange this formula into: ",
  "scores-faq-baseline-math__content_8": "The formula for a multiple choice question with N options is:",
  "scores-faq-baseline-math__content_9": "For continuous questions, the",
  "scores-faq-baseline-math__content_10": " was chosen empirically so that continuous scores have roughly the same average as binary scores. The formula for a continuous question is:",
  "scores-faq-baseline-math__content_11": "Where",
  "scores-faq-baseline-math__content_12": "is the natural logarithm, ",
  "scores-faq-baseline-math__content_13": "is the probability predicted for the outcome that actually happened, and ",
  "scores-faq-baseline-math__content_14": "is the value of the predicted probability density function at the outcome.",
  "scores-faq-baseline-math__content_15": "The continuous",
  "scores-faq-baseline-math__content_16": "depends on whether the question has open or closed bounds:",
  "scores-faq-baseline-math__content_17": "If both bounds are closed, the ",
  "scores-faq-baseline-math__content_18": "is 1, corresponding to a uniform distribution in range.",
  "scores-faq-baseline-math__content_19": "If one bound is open, the",
  "scores-faq-baseline-math__content_20": " is 0.95, corresponding to a uniform distribution in range + 5% probability out of the open bound.",
  "scores-faq-baseline-math__content_21": "If both bounds are open, the ",
  "scores-faq-baseline-math__content_22": "is 0.9, corresponding to a uniform distribution in range + 5% probability out of each open bound.",
  "scores-faq-further-math__content_1": "If you have an intuition that something should be 0 and not positive, you are correct! The average Peer score across all users is guaranteed to be 0. This does not imply that the score of the average (or median) forecast is 0: the score of the mean is not the mean of the scores.",
  "scores-faq-further-math__content_2": "There is another reason why the Peer score of the Community Prediction is positive: you can rearrange the Peer score formula to show that it is the difference between the forecaster log score and the log score of the geometric mean of all other forecasters. Since the median will be higher than the geometric mean in most cases, it follows that the score of the Community Prediction will be positive in most cases.",
  "scores-faq-peer-math__content_1": "The Peer scores are built on ",
  "scores-faq-peer-math__content_2": "log scores",
  "scores-faq-peer-math__content_3": ", with the general form:",
  "scores-faq-peer-math__content_4": "Where",
  "scores-faq-peer-math__content_5": "is the scored prediction, ",
  "scores-faq-peer-math__content_6": "is the number of other predictions, and",
  "scores-faq-peer-math__content_7": "is the i-th other prediction.",
  "scores-faq-peer-math__content_8": "Note that this can be rearranged into:",
  "scores-faq-peer-math__content_9": "Where ",
  "scores-faq-peer-math__content_10": " is the geometric mean of all other predictions.",
  "scores-faq-peer-math__content_11": "As before, for binary questions",
  "scores-faq-peer-math__content_12": "is the probability given to the correct outcome (Yes or No), for multiple choice questions it is the probability given to the option outcome that resolved Yes, and for continuous questions it is the value of the predicted pdf at the outcome.",
  "scores-faq-points-math__content_1": "Your score",
  "scores-faq-points-math__content_2": "at any given time ",
  "scores-faq-points-math__content_3": "is the sum of an \"absolute\" component and a \"relative\" component:",
  "scores-faq-points-math__content_4": "where:",
  "scores-faq-points-math__content_5": "is the outcome of the question: 1 if the question resolves positive, 0 if it resolves negative.",
  "scores-faq-points-math__content_6": "is the number of forecasters on the question.",
  "scores-faq-points-math__content_7": "is the log score relative to a 50% prior, defined as:",
  "scores-faq-points-math__content_8": "is the betting score and represents a bet placed against every other forecaster. It is described under \"constant pool scoring\" on the Metaculus scoring demo (but with a modification that for computational efficiency, the full distribution of other forecaster predictions is replaced by a fitted ",
  "scores-faq-points-math__content_9": "beta distribution",
  "scores-faq-points-math__content_10": "and ",
  "scores-faq-points-math__content_11": "depend on ",
  "scores-faq-points-math__content_12": "only and define how the points scale with the number of forecasters.",
  "scores-faq-points-math__content_13": "Note that",
  "scores-faq-points-math__content_14": ", and ",
  "scores-faq-points-math__content_15": "can all depend on ",
  "scores-faq-points-math__content_16": "and contribute to the time-dependence of",
  "scores-faq-points-math__content_17": "Your final score is given by the integral of ",
  "scores-faq-points-math__content_18": "over ",
  "scores-faq-points-math__content_19": "where",
  "scores-faq-points-math__content_20": "and ",
  "scores-faq-points-math__content_21": "are the opening and closing times. (Note that",
  "scores-faq-points-math__content_22": "between the opening time and your first prediction, and is also zero after question resolution but before question close, in the case when a question resolves early.)",
  "scores-faq-points-math__content_23": "Before May 2022, there was also a 50% point bonus given at the time the question closes, but it was discontinued and the points multiplied by 1.5 henceforth.",
  "scores-faq-truncation-example__content_1": "This example uses the Baseline score, which will be noted ",
  "scores-faq-truncation-example__content_2": ", but results would be equivalent with any proper score.",
  "scores-faq-truncation-example__content_3": "Alex wants to predict if they will be fired this year. They have a performance review scheduled this week. They estimate there is a ",
  "scores-faq-truncation-example__content_4": "chance they fail it, and if so they will be fired on the spot. If they don’t fail this week, there is still a",
  "scores-faq-truncation-example__content_5": "chance they will be fired at the end of the year. A proper scoring rule ensures that the best strategy on this question is to predict ",
  "scores-faq-truncation-example__content_6": "this week, and then",
  "scores-faq-truncation-example__content_7": "for the other 51 weeks (if they weren’t fired).",
  "scores-faq-truncation-example__content_8": "Without truncation",
  "scores-faq-truncation-example__content_9": "Without truncation, this honest strategy gives Baseline scores of:",
  "scores-faq-truncation-example__content_10": "in the ",
  "scores-faq-truncation-example__content_11": "of cases they are fired this week.",
  "scores-faq-truncation-example__content_12": " in the",
  "scores-faq-truncation-example__content_13": "of cases they are fired at the end of the year.",
  "scores-faq-truncation-example__content_14": " in the",
  "scores-faq-truncation-example__content_15": "of cases they are not fired.",
  "scores-faq-truncation-example__content_16": "For an average score of ",
  "scores-faq-truncation-example__content_17": " in expectation.",
  "scores-faq-truncation-example__content_18": "But the strategy of “predicting close to 100% in the beginning and lower later”, let's say 99% today, then 5% the other 6 days, without truncation gives Baseline scores of:",
  "scores-faq-truncation-example__content_19": "in the ",
  "scores-faq-truncation-example__content_20": "of cases they are fired this week.",
  "scores-faq-truncation-example__content_21": " in the",
  "scores-faq-truncation-example__content_22": "of cases they are fired at the end of the year.",
  "scores-faq-truncation-example__content_23": " in the",
  "scores-faq-truncation-example__content_24": "of cases they are not fired.",
  "scores-faq-truncation-example__content_25": "For an average score of ",
  "scores-faq-truncation-example__content_26": " in expectation.",
  "scores-faq-truncation-example__content_27": "Notice that",
  "scores-faq-truncation-example__content_28": ", so without truncation, the gaming strategy gives you a score almost twice as high in expectation! It is really not proper.",
  "scores-faq-truncation-example__content_29": "With truncation",
  "scores-faq-truncation-example__content_30": "With truncation, the honest strategy gives Baseline scores of:",
  "scores-faq-truncation-example__content_31": "in the",
  "scores-faq-truncation-example__content_32": "of cases they are fired this week.",
  "scores-faq-truncation-example__content_33": " in the",
  "scores-faq-truncation-example__content_34": "of cases they are fired at the end of the year.",
  "scores-faq-truncation-example__content_35": " in the",
  "scores-faq-truncation-example__content_36": "of cases they are not fired.",
  "scores-faq-truncation-example__content_37": "For an average score of ",
  "scores-faq-truncation-example__content_38": " in expectation.",
  "scores-faq-truncation-example__content_39": "While the gaming strategy gives:",
  "scores-faq-truncation-example__content_40": "in the",
  "scores-faq-truncation-example__content_41": "of cases they are fired this week.",
  "scores-faq-truncation-example__content_42": " in the",
  "scores-faq-truncation-example__content_43": "of cases they are fired at the end of the year.",
  "scores-faq-truncation-example__content_44": " in the",
  "scores-faq-truncation-example__content_45": "of cases they are not fired.",
  "scores-faq-truncation-example__content_46": "For an average score of ",
  "scores-faq-truncation-example__content_47": " in expectation.",
  "scores-faq-truncation-example__content_48": "This time,",
  "scores-faq-truncation-example__content_49": ", so with truncation, the gaming strategy gives you a worse score than the honest strategy! Which is proper."
}